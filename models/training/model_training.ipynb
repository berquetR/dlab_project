{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7524c8ab",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82927fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d417936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45f7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497de8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-40GB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab82483",
   "metadata": {},
   "source": [
    "### Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea69948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"berquetR/dlab_project_optimal_links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9342a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32c641f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'],\n",
       "    num_rows: 78088\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a91835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'],\n",
       "    num_rows: 26178\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107df14",
   "metadata": {},
   "source": [
    "### Build Prompt Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d27612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instruction_format(examples):\n",
    "    instructions = [\n",
    "        f\"Given two Wikipedia pages, a source page : {current_page} and a target page : {target}, your goal is to reach the target page starting from the source page in as few clicks as possible.\"\n",
    "        for current_page, source, target in zip(examples['current_page'], examples['source'], examples['target'])\n",
    "    ]\n",
    "    inputs = examples['current_page_links']\n",
    "    outputs = examples['next_page']\n",
    "\n",
    "    return {'instruction': instructions, 'input': inputs, 'output': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7066def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before building prompt, build instruction, input, output format for the alpaca format\n",
    "columns_to_remove = ['source', 'target', 'current_page', 'current_page_links', 'next_page','__index_level_0__' ]\n",
    "train_dataset = train_dataset.map(transform_instruction_format, batched=True, remove_columns=columns_to_remove)\n",
    "validation_dataset = validation_dataset.map(transform_instruction_format, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d25bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{} \n",
    "\n",
    "\n",
    "### Constraint:\n",
    "You are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\n",
    "\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22829e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) \n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts,}\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c91abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove_2 = ['instruction', 'input', 'output']\n",
    "train_dataset=train_dataset.map(formatting_prompts_func, batched = True, remove_columns= columns_to_remove_2)\n",
    "validation_dataset=validation_dataset.map(formatting_prompts_func, batched = True, remove_columns= columns_to_remove_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c777e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Nationalism and a target page : Antwerp, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Human\\', \\'Politics\\', \\'History of the world\\', \\'Denmark\\', \\'Culture\\', \\'Music\\', \\'Literature\\', \\'Folklore\\', \\'Mythology\\', \\'Religion\\', \\'Cultural identity\\', \\'Language\\', \\'Culture\\', \\'Ethnic group\\', \\'Gender\\', \\'Education\\', \\'Ottoman Empire\\', \\'Russia\\', \\'China\\', \\'Vatican City\\', \\'Roman Catholic Church\\', \\'Ethics\\', \\'War\\', \\'Marxism\\', \\'Socialism\\', \\'Sociology\\', \\'World War II\\', \\'French Revolution\\', \\'Napoleon\\', \\'Capitalism\\', \\'Mythology\\', \\'French Revolution\\', \\'Liberalism\\', \\'Ottoman Empire\\', \\'Spain\\', \\'South America\\', \\'Netherlands\\', \\'England\\', \\'Asia\\', \\'India\\', \\'Mahatma Gandhi\\', \\'China\\', \\'Japan\\', \\'World War I\\', \\'Ottoman Empire\\', \\'Woodrow Wilson\\', \\'Iraq\\', \\'Lebanon\\', \\'Syria\\', \\'Russian Revolution of 1917\\', \\'Ireland\\', \\'Northern Ireland\\', \\'Fascism\\', \\'Nazism\\', \\'Italy\\', \\'Germany\\', \\'World War II\\', \\'The Holocaust\\', \\'Africa\\', \\'Somalia\\', \\'Portugal\\', \\'Mozambique\\', \\'Angola\\', \\'Belarus\\', \\'Ukraine\\', \\'Moldova\\', \\'Kazakhstan\\', \\'Turkmenistan\\', \\'Uzbekistan\\', \\'Tajikistan\\', \\'Kyrgyzstan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Georgia (country)\\', \\'Latvia\\', \\'Estonia\\', \\'Lithuania\\', \\'Yugoslavia\\', \\'Northern Ireland\\', \\'European Union\\', \\'Globalization\\', \"People\\'s Republic of China\", \\'European Union\\', \\'French Revolution\\', \\'19th century\\', \\'Literacy\\', \\'Newspaper\\', \\'Book\\', \\'Ireland\\', \\'India\\', \\'Hebrew language\\', \\'United States\\', \\'German language\\', \\'World War I\\', \\'French language\\', \\'French language\\', \\'Spanish language\\', \\'English language\\', \\'Arabic language\\', \\'Algeria\\', \\'Western Sahara\\', \\'Nobiin language\\', \\'Egypt\\', \\'Sudan\\', \\'Morocco\\', \\'Civil society\\', \\'Citizenship\\', \\'Jean-Jacques Rousseau\\', \\'Liberalism\\', \\'Race\\', \\'Romanticism\\', \\'Folklore\\', \\'Brothers Grimm\\', \\'Culture\\', \\'Taiwan\\', \\'Fascism\\', \\'Liberalism\\', \\'Religion\\', \\'Ireland\\', \\'18th century\\', \\'Northern Ireland\\', \\'India\\', \\'Hinduism\\', \\'Zionism\\', \\'World War I\\', \\'Treaty of Versailles\\', \\'Lithuania\\', \\'Fascism\\', \\'Socialism\\', \\'Nazism\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'Nazism\\', \\'Race\\', \\'Nazi Germany\\', \\'Ottoman Empire\\', \\'Liberalism\\', \\'War\\', \\'Germany\\', \\'Marxism\\', \\'World War I\\', \\'Politics\\', \\'Anarchism\\', \\'Race\\', \\'Islam\\', \\'Government\\', \\'Islam\\', \\'United Nations\\']\\n\\n\\n### Response:\\nWorld_War_II'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79255210",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3957783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d58b6",
   "metadata": {},
   "source": [
    "### Model import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b337a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae549550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:59<00:00, 19.76s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer_mistral.pad_token = tokenizer.eos_token\n",
    "tokenizer_mistral.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc80319",
   "metadata": {},
   "outputs": [],
   "source": [
    "base__smaller_model_id = \"microsoft/phi-1_5\"\n",
    "small_model = AutoModelForCausalLM.from_pretrained(base__smaller_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9539b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"microsoft/phi-1_5\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "small_tokenizer.pad_token = small_tokenizer.eos_token\n",
    "small_tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2bcd4",
   "metadata": {},
   "source": [
    "### Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bdb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "small_model.gradient_checkpointing_enable()\n",
    "small_model = prepare_model_for_kbit_training(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1d2bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "#model = get_peft_model(model, config)\n",
    "small_model = get_peft_model(small_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e033281",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    print('parallelizable')\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a03d5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd7104bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = accelerator.prepare_model(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f88eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './mistral-first_run_dlab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b856d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir = output_dir, \n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        \n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-5,\n",
    "    \n",
    "        #Keep precision at 32bit for training\n",
    "        bf16 = True,\n",
    "        fp16 = False, \n",
    "        #Perform eval every save_steps\n",
    "        do_eval = True,\n",
    "        #Saving based on number of steps\n",
    "        save_strategy = 'steps',\n",
    "        #Number of steps model checkpoints will be saved and evaluated\n",
    "        save_steps = 500,\n",
    "        eval_steps = 500,\n",
    "        #Start reporting loss\n",
    "        logging_steps = 1,\n",
    "        #Use paged optimizer for memory efficiency\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "        #2 epochs\n",
    "        num_train_epochs = 2, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2be94091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_arguments_test = TrainingArguments(\n",
    "        output_dir = output_dir, \n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        \n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-5,\n",
    "    \n",
    "        #Keep precision at 32bit for training\n",
    "        bf16 = True,\n",
    "        #Perform eval every save_steps\n",
    "        do_eval = True,\n",
    "        #Saving based on number of steps\n",
    "        save_strategy = 'steps',\n",
    "        evaluation_strategy = 'steps',\n",
    "        #Number of steps model checkpoints will be saved and evaluated\n",
    "        save_steps = 10,\n",
    "        eval_steps = 10,\n",
    "        max_steps = 30,\n",
    "        #Start reporting loss\n",
    "        logging_steps = 1,\n",
    "        #Use paged optimizer for memory efficiency\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "859f4934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26178/26178 [00:12<00:00, 2132.74 examples/s]\n",
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = small_model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset= validation_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    args = training_arguments,\n",
    "    peft_config = config,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d505e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='9762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/9762 00:15 < 21:12:29, 0.13 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.636600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "392464c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78088/78088 [03:57<00:00, 329.09 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26178/26178 [01:37<00:00, 268.42 examples/s]\n",
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_small = SFTTrainer(\n",
    "    model = small_model,\n",
    "    tokenizer = small_tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset= validation_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    args = training_arguments,\n",
    "    peft_config = config,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(small_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0afb8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='9762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/9762 00:04 < 6:24:49, 0.42 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.892300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_small\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/transformers/trainer.py:2120\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_small.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083fe5c",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8ea7bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('mistral-first_run_dlab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e29957",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b66fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00dbdf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(transform_instruction_format, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3025ac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Given two Wikipedia pages, a source page : History_of_computing_hardware and a target page : I_Want_to_Hold_Your_Hand, your goal is to reach the target page starting from the source page in as few clicks as possible.',\n",
       " 'input': \"['History of the Internet', 'Clay', 'Abacus', 'Computers', 'Abacus', 'China', 'Abacus', 'Abacus', 'Johannes Kepler', 'Blaise Pascal', 'Gottfried Leibniz', 'Leibniz', 'Charles Babbage', 'Moon', 'Charles Babbage', 'United States Constitution', 'World War II', 'Computer programming', 'Charles Babbage', 'Richard Feynman', 'Integrated circuit', 'World War II', 'Computer programming', 'World War II', 'Electronics', 'Mercury (element)', 'Alan Turing', 'Alan Turing', 'Computer science', 'Algorithm', 'Germany', 'Charles Babbage', 'Programming language', 'World War II', 'World War II', 'World War II', 'Alan Turing', 'Boolean logic', 'John von Neumann', 'Electronics', 'University of Cambridge', 'Soviet Union', 'Ukraine', 'Programming language', 'Central processing unit', 'Integrated circuit']\",\n",
       " 'output': 'World_War_II'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a2399",
   "metadata": {},
   "source": [
    "### Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69642bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bfa76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-first_run_dlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24063660",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f413f50",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7ebb112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : History_of_computing_hardware and a target page : I_Want_to_Hold_Your_Hand, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'History of the Internet\\', \\'Clay\\', \\'Abacus\\', \\'Computers\\', \\'Abacus\\', \\'China\\', \\'Abacus\\', \\'Abacus\\', \\'Johannes Kepler\\', \\'Blaise Pascal\\', \\'Gottfried Leibniz\\', \\'Leibniz\\', \\'Charles Babbage\\', \\'Moon\\', \\'Charles Babbage\\', \\'United States Constitution\\', \\'World War II\\', \\'Computer programming\\', \\'Charles Babbage\\', \\'Richard Feynman\\', \\'Integrated circuit\\', \\'World War II\\', \\'Computer programming\\', \\'World War II\\', \\'Electronics\\', \\'Mercury (element)\\', \\'Alan Turing\\', \\'Alan Turing\\', \\'Computer science\\', \\'Algorithm\\', \\'Germany\\', \\'Charles Babbage\\', \\'Programming language\\', \\'World War II\\', \\'World War II\\', \\'World War II\\', \\'Alan Turing\\', \\'Boolean logic\\', \\'John von Neumann\\', \\'Electronics\\', \\'University of Cambridge\\', \\'Soviet Union\\', \\'Ukraine\\', \\'Programming language\\', \\'Central processing unit\\', \\'Integrated circuit\\']\\n\\n\\n### Response:\\nThe link \\'Computers\\' is the best choice to reach the target page \\'I_Want_to_Hold_Your_Hand\\' in as few clicks as possible.\\n\\n\\nExplanation:\\nThe target page \\'I_Want_to_Hold_Your_Hand\\'']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : World_War_II and a target page : I_Want_to_Hold_Your_Hand, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Battle of Normandy\\', \\'Nuclear weapon\\', \\'United States\\', \\'Soviet Union\\', \\'Europe\\', \\'Cold War\\', \\'United Kingdom\\', \\'United States\\', \\'Nazi Germany\\', \\'Joseph Stalin\\', \\'Winston Churchill\\', \\'Adolf Hitler\\', \\'Benito Mussolini\\', \\'War\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'Poland\\', \\'Soviet Union\\', \\'North Africa\\', \\'Royal Air Force\\', \\'Royal Navy\\', \\'Soviet Union\\', \\'Battle of Stalingrad\\', \\'Battle of Normandy\\', \\'Battle of the Bulge\\', \\'Jews\\', \\'The Holocaust\\', \\'Attack on Pearl Harbor\\', \\'Battle of Midway\\', \\'Battle of Leyte Gulf\\', \\'Benito Mussolini\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'China\\', \\'United States\\', \\'United Kingdom\\', \\'Treaty of Versailles\\', \\'Japan\\', \\'United States\\', \\'United Kingdom\\', \\'Republic of China\\', \\'Philippines\\', \\'Prime Minister of the United Kingdom\\', \\'Moscow\\', \\'Warsaw\\', \\'Soviet Union\\', \\'Romania\\', \\'Aircraft carrier\\', \\'Montevideo\\', \\'China\\', \\'Beijing\\', \\'Japanese war crimes\\', \\'League of Nations\\', \\'Latvia\\', \\'Lithuania\\', \\'Estonia\\', \\'Denmark\\', \\'Norway\\', \\'Luxembourg\\', \\'Belgium\\', \\'Netherlands\\', \\'France\\', \\'Flanders\\', \\'Blitzkrieg\\', \\'English Channel\\', \\'Paris\\', \\'Royal Air Force\\', \\'Battle of Britain\\', \\'Luftwaffe\\', \\'The Blitz\\', \\'Supermarine Spitfire\\', \\'Albania\\', \\'Gibraltar\\', \\'Malta\\', \\'Libya\\', \\'Egypt\\', \\'Suez Canal\\', \\'United Kingdom\\', \\'India\\', \\'New Zealand\\', \\'Erwin Rommel\\', \\'United Kingdom\\', \\'United States Congress\\', \\'Yugoslavia\\', \\'Battle of Moscow\\', \\'Winston Churchill\\', \\'Iraq\\', \\'Syria\\', \\'Lebanon\\', \\'Damascus\\', \\'Royal Navy\\', \\'Imperial Japanese Navy\\', \\'Saigon\\', \\'Winston Churchill\\', \\'Hong Kong\\', \\'British Empire\\', \\'Myanmar\\', \\'Singapore\\', \\'Battle of Normandy\\', \\'Suez Canal\\', \\'Tunisia\\', \\'Battle of Midway\\', \\'Naval Battle of Guadalcanal\\', \\'Benito Mussolini\\', \\'Rome\\', \\'Royal Navy\\', \\'Solomon Islands\\', \\'Mao Zedong\\', \\'Rome\\', \\'Hungary\\', \\'Romania\\', \\'Bulgaria\\', \\'Winston Churchill\\', \\'Bucharest\\', \\'Battle of Normandy\\', \\'United States\\', \\'United Kingdom\\', \\'Canada\\', \\'Battle of Normandy\\', \\'Paris\\', \\'Antwerp\\', \\'Battle of the Bulge\\', \\'Marshall Islands\\', \\'Guam\\', \\'Battle of Leyte Gulf\\', \\'Franklin D. Roosevelt\\', \\'Winston Churchill\\', \\'Joseph Stalin\\', \\'United Nations\\', \\'Election\\', \\'Rhine\\', \\'Baltic Sea\\', \\'Milan\\', \\'Great Britain\\', \\'Nuclear weapon\\', \\'Harry S. Truman\\', \\'Nuclear weapon\\', \\'Nuclear weapon\\', \\'North Korea\\', \\'Mao Zedong\\', \\'Hirohito\\', \\'The Holocaust\\', \\'Nazism\\', \\'League of Nations\\', \\'Japan\\', \\'Hirohito\\', \\'Slavery\\', \\'China\\', \\'Austria\\', \\'Lithuania\\', \\'Latvia\\', \\'Estonia\\', \\'Armia Krajowa\\', \\'Communism\\', \\'Enigma machine\\', \\'Nuclear weapon\\', \\'Radar\\', \\'Jet engine\\', \\'Aircraft\\', \\'Tank\\', \\'Finland\\', \\'United States of America\\', \\'United Nations\\', \\'San Francisco, California\\', \\'Philippines\\', \\'Cold War\\', \\'Korea\\', \"People\\'s Republic of China\", \\'Germany\\', \\'Austria\\', \\'Japan\\', \\'Korea\\', \\'Marshall Plan\\', \\'Estonia\\', \\'Latvia\\', \\'Lithuania\\', \\'Taiwan\\', \\'Philippines\\', \\'India\\', \\'Pakistan\\', \\'Vietnam\\', \\'League of Nations\\', \\'United Nations\\', \\'Israel\\']\\n\\n\\n### Response:\\nThe link \\'Iraq\\' could be a possible link to reach the target page \\'I_Want_to_Hold_Your_Hand\\' as it is the only link in the input that has the word \\'hand\\' in it. However, it is not guaranteed that this link will lead to the']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : United_Kingdom and a target page : I_Want_to_Hold_Your_Hand, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'French language\\', \\'London\\', \\'English language\\', \\'List of countries by system of government\\', \\'Constitutional monarchy\\', \\'British monarchy\\', \\'Elizabeth II of the United Kingdom\\', \\'Prime Minister of the United Kingdom\\', \\'Tony Blair\\', \\'Acts of Union 1707\\', \\'European Union\\', \\'Currency\\', \\'Pound sterling\\', \\'Time zone\\', \\'Scottish Gaelic language\\', \\'Scots language\\', \\'Latin\\', \\'English language\\', \\'Scottish Gaelic language\\', \\'Great Britain\\', \\'European Union\\', \\'Europe\\', \\'Great Britain\\', \\'Northern Ireland\\', \\'Ireland\\', \\'Atlantic Ocean\\', \\'North Sea\\', \\'English Channel\\', \\'Irish Sea\\', \\'France\\', \\'Republic of Ireland\\', \\'England\\', \\'Scotland\\', \\'Wales\\', \\'Northern Ireland\\', \\'Bermuda\\', \\'Gibraltar\\', \\'Montserrat\\', \\'Saint Helena\\', \\'Channel Islands\\', \\'Isle of Man\\', \\'Constitutional monarchy\\', \\'Elizabeth II of the United Kingdom\\', \\'United States dollar\\', \\'European Union\\', \\'NATO\\', \\'United Nations\\', \\'British Empire\\', \\'English language\\', \\'Northern Ireland\\', \\'Wales\\', \\'Laws in Wales Acts 1535-1542\\', \\'England\\', \\'Republic of Ireland\\', \\'Capitalism\\', \\'Parliamentary system\\', \\'British Empire\\', \\'Earth\\', \\'World War I\\', \\'World War II\\', \\'British Empire\\', \\'European Union\\', \\'Constitutional monarchy\\', \\'British monarchy\\', \\'Prime Minister of the United Kingdom\\', \\'Parliament of the United Kingdom\\', \\'British House of Commons\\', \\'House of Lords\\', \\'Tony Blair\\', \\'Buckingham Palace\\', \\'Anne of Great Britain\\', \\'Elizabeth II of the United Kingdom\\', \\'Parliament of the United Kingdom\\', \\'British House of Commons\\', \\'House of Lords\\', \\'Church of England\\', \\'Palace of Westminster\\', \\'River Thames\\', \\'London\\', \\'Parliament of the United Kingdom\\', \\'Republic of Ireland\\', \\'Belfast\\', \\'Scotland\\', \\'House of Lords\\', \\'England\\', \\'Wales\\', \\'Northern Ireland\\', \\'Scotland\\', \\'Lake District\\', \\'Peak District\\', \\'Cotswolds\\', \\'France\\', \\'England\\', \\'Lake District\\', \\'Scottish Highlands\\', \\'Ben Nevis\\', \\'Loch\\', \\'Hebrides\\', \\'Orkney Islands\\', \\'Edinburgh\\', \\'World Heritage Site\\', \\'Glasgow\\', \\'Cardiff\\', \\'Belfast\\', \"Giant\\'s Causeway\", \\'British Isles\\', \\'Northern Ireland\\', \\'England\\', \\'Wales\\', \\'Scotland\\', \\'Northern Ireland\\', \\'London\\', \\'Edinburgh\\', \\'Cardiff\\', \\'Belfast\\', \\'Trafalgar Square\\', \\'London\\', \\'European Union\\', \\'Germany\\', \\'France\\', \\'Europe\\', \\'British Empire\\', \\'Italy\\', \\'Spain\\', \\'English language\\', \\'British Isles\\', \\'Canada\\', \\'Argentina\\', \\'Scots language\\', \\'Hindi\\', \\'Hindi\\', \\'Canterbury Cathedral\\', \\'Roman Britain\\', \\'Church of England\\', \\'Henry VIII of England\\', \\'House of Lords\\', \\'Canterbury Cathedral\\', \\'Archbishop of Canterbury\\', \\'Westminster Abbey\\', \\'Church of England\\', \\'Anglican Communion\\', \\'Church of Ireland\\', \\'Hinduism\\', \\'Europe\\', \\'Christianity\\', \\'Islam\\', \\'Hinduism\\', \\'Sikhism\\', \\'Judaism\\', \\'Pakistan\\', \\'India\\', \\'Bangladesh\\', \\'Somalia\\', \\'India\\', \\'Hinduism\\', \\'Sikhism\\', \\'Market\\', \\'Germany\\', \\'Industrial Revolution\\', \\'Steel\\', \\'HSBC\\', \\'Edinburgh\\', \\'Europe\\', \\'BAE Systems\\', \\'Airbus\\', \\'GlaxoSmithKline\\', \\'Natural gas\\', \\'Petroleum\\', \\'Pound sterling\\', \\'Euro\\', \\'Gordon Brown\\', \\'Tony Blair\\', \\'Local government in the United Kingdom\\', \\'England\\', \\'Kingdom\\', \\'Scotland\\', \\'Kingdom\\', \\'Wales\\', \\'Principality\\', \\'Northern Ireland\\', \\'London\\', \\'City status in the United Kingdom\\', \\'Jersey\\', \\'Guernsey\\', \\'Isle of Man\\', \\'British Empire\\', \\'Royal Navy\\', \\'Elizabeth II of the United Kingdom\\', \\'Royal Air Force\\', \\'Royal Navy\\', \\'Royal Marines\\', \\'NATO\\', \\'University of Cambridge\\', \\'Bertrand Russell\\', \\'Adam Smith\\', \\'James Clerk Maxwell\\', \\'Michael Faraday\\', \\'Charles Darwin\\', \\'Francis Crick\\', \\'Isambard Kingdom Brunel\\', \\'Hydrogen\\', \\'Gravity\\', \\'Electron\\', \\'DNA\\', \\'Television\\', \\'Jet engine\\', \\'Bicycle\\', \\'Computer\\', \\'William Shakespeare\\', \\'William Shakespeare\\', \\'Novel\\', \\'Jane Austen\\', \\'Charles Dickens\\', \\'J. R. R. Tolkien\\', \\'J. K. Rowling\\', \\'Ben Jonson\\', \\'John Milton\\', \\'T. S. Eliot\\', \\'Christopher Wren\\', \\'Henry Purcell\\', \\'Benjamin Britten\\', \\'Opera\\', \\'The Beatles\\', \\'Queen (band)\\', \\'Iron Maiden\\', \\'The Rolling Stones\\', \\'Oasis (band)\\', \\'Kylie Minogue\\', \\'Arctic Monkeys\\', \\'J. M. W. Turner\\', \\'The Championships, Wimbledon\\', \\'Grand Slam (tennis)\\', \\'Sport\\', \\'Association football\\', \\'Rugby football\\', \\'Cricket\\', \\'Tennis\\', \\'Football (soccer)\\', \\'Olympic Games\\', \\'Manchester United F.C.\\', \\'Liverpool F.C.\\', \\'Chelsea F.C.\\', \\'Arsenal F.C.\\', \\'Celtic F.C.\\', \\'Liverpool F.C.\\', \\'Manchester United F.C.\\', \\'Celtic F.C.\\', \\'Cricket\\', \\'England\\', \\'England\\', \\'England\\', \\'England\\', \\'England\\', \\'Cricket\\', \\'Olympic Games\\', \\'Wales\\', \\'Northern Ireland\\', \\'Republic of Ireland\\', \\'Rugby World Cup\\', \\'Tennis\\', \\'Charles II of England\\', \\'Ireland\\', \\'Scottish Highlands\\', \\'Formula One\\', \\'Damon Hill\\', \\'Plymouth\\', \\'Lion\\', \\'Wales\\', \\'Great Britain\\']\\n\\n\\n### Response:\\nThe link \\'Ireland\\' is the closest link to the target page \\'I_Want_to_Hold_Your_Hand\\' as it is the only link that directly connects to the target page.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : The_Beatles and a target page : I_Want_to_Hold_Your_Hand, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Paul McCartney\\', \\'Liverpool\\', \\'England\\', \\'Paul McCartney\\', \\'Liverpool\\', \\'Paul McCartney\\', \\'Popular culture\\', \\'20th century\\', \\'United Kingdom\\', \\'The Beatles discography\\', \\'United States\\', \\'Manchester\\', \\'Chicago\\', \\'I Want to Hold Your Hand\\', \\'Europe\\', \\'North America\\', \\'Australia\\', \\'New Zealand\\', \\'Adelaide\\', \\'Kansas City, Missouri\\', \\'Elizabeth II of the United Kingdom\\', \\'Harold Wilson\\', \\'BBC\\', \\'Philippines\\', \\'Christianity\\', \\'United States\\', \\'South Africa\\', \\'Chicago\\', \"Sgt. Pepper\\'s Lonely Hearts Club Band\", \\'India\\', \\'Communism\\', \\'BBC\\', \\'Radio\\', \\'United States\\', \\'Liverpool\\', \\'Elvis Presley\\', \\'Ragtime\\', \"Sgt. Pepper\\'s Lonely Hearts Club Band\", \\'Bob Dylan\\', \\'I Want to Hold Your Hand\\', \\'Johann Sebastian Bach\\', \\'United States\\', \\'Nirvana (band)\\', \\'Kurt Cobain\\', \\'U2\\', \\'Oasis (band)\\', \\'Oasis (band)\\', \\'Beatles for Sale\\', \"Sgt. Pepper\\'s Lonely Hearts Club Band\", \\'CD\\', \\'Sony\\', \\'United States\\', \\'Italy\\', \\'United Kingdom\\', \\'Ireland\\', \\'Finland\\', \\'Norway\\', \\'Sweden\\', \\'Canada\\', \\'Denmark\\', \\'I Want to Hold Your Hand\\', \\'Australia\\', \\'Germany\\', \\'Netherlands\\', \\'Argentina\\', \\'Hong Kong\\', \\'Malaysia\\', \\'Luxembourg\\', \\'Uruguay\\', \\'New Zealand\\', \\'Spain\\', \\'Switzerland\\', \\'Austria\\', \\'Hey Jude\\', \\'BBC\\', \\'Stonehenge\\', \\'Bahamas\\', \\'James Bond\\', \\'World War II\\', \\'BBC\\']\\n\\n\\n### Response:\\nThe link \\'I Want to Hold Your Hand\\' is present in the Input, so it is the link that should be clicked to reach the target page in as few clicks as possible.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Body_mass_index and a target page : Mercury_(element), your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Belgium\\', \\'World Health Organization\\', \\'World Health Organization\\', \\'Rugby football\\']\\n\\n\\n### Response:\\nThe target page Mercury_(element) cannot be reached directly from the given links in the input.\\n\\n\\n### Explanation:\\nThe links provided in the input are [\\'Belgium\\', \\'World Health Organization\\', \\'World Health Organization\\', \\'Rugby football\\']. None of these links leads to the']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : World_Health_Organization and a target page : Mercury_(element), your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Geneva\\', \\'Switzerland\\', \\'English language\\', \\'French language\\', \\'Spanish language\\', \\'United Nations\\', \\'Geneva\\', \\'Switzerland\\', \\'League of Nations\\', \\'Malaria\\', \\'AIDS\\', \\'Smallpox\\', \\'Malaria\\', \\'Zimbabwe\\', \\'Tobacco\\', \\'Sugar\\', \\'Liechtenstein\\', \\'Niue\\', \\'Cook Islands\\', \\'Puerto Rico\\', \\'Tokelau\\', \\'Vatican City\\', \\'Republic of China\\', \\'Taiwan\\', \"People\\'s Republic of China\", \\'Niue\\', \\'United States of America\\', \\'Mediterranean\\', \\'Cairo\\', \\'Egypt\\', \\'Copenhagen\\', \\'Denmark\\', \\'New Delhi\\', \\'India\\', \\'Manila\\', \\'Philippines\\', \\'Egypt\\', \\'Sudan\\', \\'Tunisia\\', \\'Morocco\\', \\'Somalia\\', \\'Copenhagen\\', \\'Denmark\\', \\'New Delhi\\', \\'India\\', \\'Cairo\\', \\'Egypt\\', \\'Maghreb\\', \\'Manila\\', \\'Philippines\\', \\'Oceania\\', \\'South Korea\\', \\'United States of America\\', \\'Canada\\', \\'Brazil\\', \\'Denmark\\', \\'Japan\\', \\'Norway\\', \\'South Korea\\', \\'Sweden\\', \\'Hong Kong\\', \"People\\'s Republic of China\", \\'United Nations\\']\\n\\n\\n### Response:\\nThe link \\'Mercury_(element)\\' is not present in the provided Input. Therefore, it is not possible to reach the target page in as few clicks as possible starting from the source page using the links provided in the Input.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Tobacco and a target page : Mercury_(element), your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Scientific classification\\', \\'Plant\\', \\'Carolus Linnaeus\\', \\'Nicotiana tabacum\\', \\'North America\\', \\'South America\\', \\'Tobacco smoking\\', \\'Asthma\\', \\'Cancer\\', \\'Stroke\\', \\'Tobacco smoking\\', \\'Europe\\', \\'South America\\', \\'United States\\', \\'Nicotiana tabacum\\', \\'Bermuda\\', \\'England\\', \\'Scotland\\', \\'Richmond, Virginia\\', \\'Protein\\', \\'Arabic language\\', \\'9th century\\', \"People\\'s Republic of China\", \\'Brazil\\', \\'India\\', \\'United States\\', \\'Indonesia\\', \\'Turkey\\', \\'Greece\\', \\'Argentina\\', \\'Italy\\', \\'Pakistan\\', \\'Seed\\', \\'Soil\\', \\'Horse\\', \\'Beetle\\', \\'United States\\', \\'Nitrogen\\', \\'Polonium\\', \\'Greece\\', \\'Tea\\', \\'Cancer\\', \\'Cyprus\\', \\'Syria\\', \\'American Civil War\\', \\'Europe\\', \\'Turkey\\', \\'Greece\\', \\'Bulgaria\\', \\'Republic of Macedonia\\', \\'Ottoman Empire\\', \\'New York City\\', \\'Mining\\', \\'United Kingdom\\', \\'19th century\\', \\'United States\\', \\'European Union\\', \\'India\\', \\'India\\', \\'India\\', \\'Ganesh\\', \\'Gardening\\']\\n\\n\\n### Response:\\nThe link \\'Element\\' under the \\'Scientific classification\\' section on the Tobacco page could lead to the Mercury_(element) page. However, it is important to note that this link may not always be present, and there is no guarantee that it will lead to the target page. It is recommended to']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Polonium and a target page : Mercury_(element), your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Bismuth\\', \\'Astatine\\', \\'Tellurium\\', \\'Ununhexium\\', \\'List of elements by name\\', \\'Color\\', \\'Xenon\\', \\'Electron\\', \\'Phase (matter)\\', \\'Magnetism\\', \\'Lead\\', \\'Bismuth\\', \\'Lead\\', \\'Bismuth\\', \\'Day\\', \\'Lead\\', \\'Chemical element\\', \\'Periodic table\\', \\'Tellurium\\', \\'Bismuth\\', \\'Uranium\\', \\'Beryllium\\', \\'Neutron\\', \\'Nuclear weapon\\', \\'Textile\\', \\'Marie Curie\\', \\'Poland\\', \\'Latin\\', \\'Bismuth\\', \\'Uranium\\', \\'Bismuth\\', \\'Neutron\\', \\'Lead\\', \\'Bismuth\\', \\'Radium\\', \\'Moon\\', \\'Russia\\']\\n\\n\\n### Response:\\nThe link \\'Periodic table\\' is the best choice to reach the target page \\'Mercury_(element)\\' in as few clicks as possible.\\n\\n\\nExplanation:\\nFrom the source page \\'Polonium\\', the link \\'Periodic table\\' is the most direct way to reach the target page']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Chemical_element and a target page : Mercury_(element), your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Chemistry\\', \\'Matter\\', \\'Atom\\', \\'Electron\\', \\'Proton\\', \\'Neutron\\', \\'Oxygen\\', \\'Hydrogen\\', \\'Helium\\', \\'Californium\\', \\'Technetium\\', \\'List of elements by name\\', \\'Periodic table\\', \\'Carbon\\', \\'English language\\', \\'Latin alphabet\\', \\'Lutetium\\', \\'Niobium\\', \\'Alchemy\\', \\'Latin alphabet\\', \\'Latin\\', \\'Sodium\\', \\'Tungsten\\', \\'Mercury (element)\\', \\'Potassium\\', \\'Antimony\\', \\'Yttrium\\', \\'Hydrogen\\', \\'Hydrogen\\', \\'Helium\\', \\'Oxygen\\', \\'Carbon\\', \\'Neon\\', \\'Iron\\', \\'Nitrogen\\', \\'Silicon\\', \\'Magnesium\\', \\'Sulfur\\', \\'Ununoctium\\', \\'Russia\\']\\n\\n\\n### Response:\\nMercury (element)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Ununhexium and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Ununpentium\\', \\'Polonium\\', \\'List of elements by name\\', \\'Color\\', \\'Radon\\', \\'Polonium\\', \\'Electron\\', \\'Phase (matter)\\', \\'Periodic table\\', \\'Polonium\\', \\'Ununoctium\\', \\'Ununoctium\\', \\'Russia\\', \\'Curium\\', \\'Calcium\\', \\'Curium\\', \\'Calcium\\', \\'Ununquadium\\', \\'Ununquadium\\', \\'Calcium\\', \\'Ununoctium\\']\\n\\n\\n### Response:\\nThe closest link to the target page \\'Oman\\' from the given Input is not present. Therefore, it is not possible to reach the target page in a single click from the source page using the provided links.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Russia and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'SOS Children in Russia\\', \\'National Anthem of Russia\\', \\'Moscow\\', \\'Russian language\\', \\'List of countries by system of government\\', \\'Soviet Union\\', \\'Currency\\', \\'Time zone\\', \\'Russian language\\', \\'Russian language\\', \\'Country\\', \\'Canada\\', \\'Norway\\', \\'Finland\\', \\'Estonia\\', \\'Latvia\\', \\'Lithuania\\', \\'Poland\\', \\'Belarus\\', \\'Ukraine\\', \\'Georgia (country)\\', \\'Azerbaijan\\', \\'Kazakhstan\\', \"People\\'s Republic of China\", \\'Mongolia\\', \\'North Korea\\', \\'United States\\', \\'Japan\\', \\'Soviet Union\\', \\'Soviet Union\\', \\'Soviet Union\\', \\'Christian Era\\', \\'Tribe\\', \\'Europe\\', \\'Khazars\\', \\'Byzantine Empire\\', \\'Kiev\\', \\'Crusades\\', \\'Volga River\\', \\'Principalities\\', \\'Ukraine\\', \\'Belarus\\', \\'Poland\\', \\'Byzantine Empire\\', \\'Moscow\\', \\'Eastern Roman Empire\\', \\'Volga River\\', \\'Amur\\', \\'North America\\', \\'Asia\\', \\'Peter I of Russia\\', \\'Sweden\\', \\'Saint Petersburg\\', \\'Black Sea\\', \\'Georgia (country)\\', \\'La Grande Armée\\', \\'Paris\\', \\'Liberalism\\', \\'Finland\\', \\'Crimean War\\', \\'France\\', \\'Ottoman Empire\\', \\'Romania\\', \\'Serbia\\', \\'Montenegro\\', \\'Bulgaria\\', \\'World War I\\', \\'Russian Revolution of 1917\\', \\'Saint Petersburg\\', \\'Moscow\\', \\'Vladimir Lenin\\', \\'Soviet Union\\', \\'Nationalism\\', \\'Georgia (country)\\', \\'Joseph Stalin\\', \\'Lenin\\', \\'Leon Trotsky\\', \\'Famine\\', \\'Nazi Germany\\', \\'Battle of Stalingrad\\', \\'Soviet Union\\', \\'Communism\\', \\'United States\\', \\'Cold War\\', \\'United Kingdom\\', \\'Sputnik 1\\', \\'Yuri Gagarin\\', \\'Earth\\', \\'Agriculture\\', \\'Cuba\\', \\'Turkey\\', \\'United Nations\\', \\'Mikhail Gorbachev\\', \\'Communist\\', \\'Russian constitutional crisis of 1993\\', \\'Civil society\\', \\'Human rights\\', \\'Mass media\\', \\'Nuclear weapons\\', \\'Government\\', \\'Moscow\\', \\'Saint Petersburg\\', \\'Arctic\\', \\'Climate\\', \\'Iceland\\', \\'Europe\\', \\'Asia\\', \\'Volcano\\', \\'Arctic Ocean\\', \\'Pacific Ocean\\', \\'Baltic Sea\\', \\'Black Sea\\', \\'Caspian Sea\\', \\'Lake Baikal\\', \\'Norway\\', \\'Finland\\', \\'Baltic Sea\\', \\'Estonia\\', \\'Latvia\\', \\'Belarus\\', \\'Ukraine\\', \\'Black Sea\\', \\'Georgia (country)\\', \\'Azerbaijan\\', \\'Caspian Sea\\', \\'Azerbaijan\\', \\'Kazakhstan\\', \\'Kazakhstan\\', \\'Mongolia\\', \\'North Korea\\', \\'Pacific Ocean\\', \\'United States\\', \\'U.S. state\\', \\'Arctic Ocean\\', \\'Poland\\', \\'Lithuania\\', \\'Baltic Sea\\', \\'Black Sea\\', \\'Denmark\\', \\'North Sea\\', \\'Istanbul\\', \\'Turkey\\', \\'Mediterranean Sea\\', \\'Suez Canal\\', \\'Caspian Sea\\', \\'Poland\\', \\'Time zone\\', \\'Moscow\\', \\'Saint Petersburg\\', \\'City\\', \\'Russian language\\', \\'Moscow\\', \\'Moscow\\', \\'Saint Petersburg\\', \\'Saint Petersburg\\', \\'Petroleum\\', \\'Natural gas\\', \\'Statistics\\', \\'Saint Petersburg\\', \\'Law\\', \\'Pacific Ocean\\', \\'Jews\\', \\'Russian language\\', \\'Christianity\\', \\'Islam\\', \\'Judaism\\', \\'Buddhism\\', \\'Norway\\', \\'Finland\\', \\'Canada\\', \\'United States\\', \\'Arctic Ocean\\', \\'United States\\', \\'Sweden\\', \\'Baltic Sea\\', \\'Estonia\\', \\'Latvia\\', \\'Lithuania\\', \\'Poland\\', \\'Belarus\\', \\'Ukraine\\', \\'Pacific Ocean\\', \\'Japan\\', \\'Georgia (country)\\', \\'Azerbaijan\\', \\'Ukraine\\', \\'Black Sea\\', \\'Romania\\', \\'Black Sea\\', \\'Bulgaria\\', \\'Black Sea\\', \\'Turkey\\', \\'Black Sea\\', \\'Caspian Sea\\', \\'Kazakhstan\\', \\'Caspian Sea\\', \\'Turkmenistan\\', \\'Caspian Sea\\', \\'Azerbaijan\\', \\'Iran\\', \\'Kazakhstan\\', \\'Mongolia\\', \"People\\'s Republic of China\", \"People\\'s Republic of China\", \\'North Korea\\', \\'Japan\\', \\'North Korea\\', \\'South Korea\\']\\n\\n\\n### Response:\\nThe link \\'Georgia (country)\\' is the closest link to the target page Oman.\\n\\n\\nExplanation:\\n\\nThe target page Oman is not directly linked from the source page Russia. However, the closest link to the target page is \\'Georgia (country)\\' as it is']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Black_Sea and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Sea\\', \\'Europe\\', \\'Atlantic Ocean\\', \\'Mediterranean Sea\\', \\'Danube\\', \\'Turkey\\', \\'Bulgaria\\', \\'Romania\\', \\'Ukraine\\', \\'Russia\\', \\'Georgia (country)\\', \\'Istanbul\\', \\'Russian language\\', \\'Homer\\', \\'Herodotus\\', \\'Red Sea\\', \\'Mediterranean Sea\\', \\'Bulgaria\\', \\'Miocene\\', \\'Caspian Sea\\', \\'Aral Sea\\', \\'Turkey\\', \\'Georgia (country)\\', \\'Mediterranean Sea\\', \\'Oxygen\\', \\'Sulfuric acid\\', \\'Pyrite\\', \\'Turkey\\', \\'Caspian Sea\\', \\'Mediterranean Sea\\', \\'Deluge (mythology)\\', \\'Caspian Sea\\', \\'Georgia (country)\\', \\'Abkhazia\\']\\n\\n\\n### Response:\\nThe link \\'Georgia (country)\\' is the best choice to reach the target page Oman in as few clicks as possible.\\n\\n\\n### Explanation:\\nThe link \\'Georgia (country)\\' is the only link among the proposed links that has a direct link to the target page O']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Turkey and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'English language\\', \\'Istanbul\\', \\'List of countries by system of government\\', \\'Currency\\', \\'Time zone\\', \\'Country\\', \\'Asia\\', \\'Europe\\', \\'Bulgaria\\', \\'Greece\\', \\'Georgia (country)\\', \\'Armenia\\', \\'Iran\\', \\'Azerbaijan\\', \\'Iraq\\', \\'Syria\\', \\'Black Sea\\', \\'Mediterranean Sea\\', \\'Democracy\\', \\'Ottoman Empire\\', \\'United Nations\\', \\'NATO\\', \\'European Union\\', \\'Asia\\', \\'Europe\\', \\'Italy\\', \\'Alexander the Great\\', \\'Constantine I\\', \\'Istanbul\\', \\'Roman Empire\\', \\'Byzantine Empire\\', \\'9th century\\', \\'10th century\\', \\'Byzantine Empire\\', \\'11th century\\', \\'Ottoman Empire\\', \\'Ottoman Empire\\', \\'Ottoman Empire\\', \\'Polish-Lithuanian Commonwealth\\', \\'World War I\\', \\'Istanbul\\', \\'World War II\\', \\'Soviet Union\\', \\'NATO\\', \\'Cyprus\\', \\'European Union\\', \\'Parliamentary system\\', \\'Government\\', \\'United States\\', \\'European Union\\', \\'Turkish Republic of Northern Cyprus\\', \\'Iraq\\', \\'Greece\\', \\'Cyprus\\', \\'United Nations\\', \\'European Union\\', \\'F-35 Lightning II\\', \\'Somalia\\', \\'Albania\\', \\'Bosnia and Herzegovina\\', \\'Kosovo\\', \\'Afghanistan\\', \\'Istanbul\\', \\'İstanbul\\', \\'Europe\\', \\'Mozambique\\', \\'Chile\\', \\'Mediterranean\\', \\'Black Sea\\', \\'Black Sea\\', \\'Black Sea\\', \\'Euphrates\\', \\'Tigris\\', \\'Climate\\', \\'Agriculture\\', \\'Transport\\', \\'China\\', \\'India\\', \\'Alcohol\\', \\'Greece\\', \\'Italy\\', \\'Spain\\', \\'Jews\\', \\'Arabic language\\', \\'Ottoman Empire\\', \\'Olive oil\\', \\'Islam\\', \\'Azeri\\', \\'Eastern Orthodox Church\\', \\'Judaism\\', \"Bahá\\'í Faith\", \\'Atheism\\', \\'Mosque\\', \\'Eastern Orthodox Church\\', \\'Istanbul\\', \\'Bulgaria\\', \\'Greece\\', \\'Bulgaria\\', \\'Romania\\', \\'Ukraine\\', \\'Russia\\', \\'Georgia (country)\\', \\'Black Sea\\', \\'Georgia (country)\\', \\'Greece\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Mediterranean Sea\\', \\'Egypt\\', \\'Syria\\', \\'Iran\\', \\'Iraq\\', \\'List of countries\\', \\'Europe\\', \\'Albania\\', \\'Andorra\\', \\'Armenia\\', \\'Austria\\', \\'Azerbaijan\\', \\'Belarus\\', \\'Belgium\\', \\'Bosnia and Herzegovina\\', \\'Bulgaria\\', \\'Croatia\\', \\'Cyprus\\', \\'Czech Republic\\', \\'Denmark\\', \\'Estonia\\', \\'Finland\\', \\'France\\', \\'Georgia (country)\\', \\'Germany\\', \\'Greece\\', \\'Hungary\\', \\'Iceland\\', \\'Republic of Ireland\\', \\'Italy\\', \\'Kazakhstan\\', \\'Latvia\\', \\'Liechtenstein\\', \\'Lithuania\\', \\'Luxembourg\\', \\'Republic of Macedonia\\', \\'Malta\\', \\'Moldova\\', \\'Monaco\\', \\'Montenegro\\', \\'Netherlands\\', \\'Norway\\', \\'Poland\\', \\'Portugal\\', \\'Romania\\', \\'Russia\\', \\'San Marino\\', \\'Serbia\\', \\'Slovakia\\', \\'Slovenia\\', \\'Spain\\', \\'Sweden\\', \\'Switzerland\\', \\'Ukraine\\', \\'United Kingdom\\', \\'Vatican City\\', \\'Asia\\', \\'List of countries\\', \\'Asia\\', \\'Afghanistan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Bahrain\\', \\'Bangladesh\\', \\'Bhutan\\', \\'Brunei\\', \\'Cambodia\\', \"People\\'s Republic of China\", \\'Cyprus\\', \\'East Timor\\', \\'Georgia (country)\\', \\'India\\', \\'Indonesia\\', \\'Iran\\', \\'Iraq\\', \\'Israel\\', \\'Japan\\', \\'Jordan\\', \\'Kazakhstan\\', \\'Kuwait\\', \\'Kyrgyzstan\\', \\'Laos\\', \\'Lebanon\\', \\'Malaysia\\', \\'Maldives\\', \\'Mongolia\\', \\'Myanmar\\', \\'Nepal\\', \\'North Korea\\', \\'Oman\\', \\'Pakistan\\', \\'Philippines\\', \\'Qatar\\', \\'Russia\\', \\'Saudi Arabia\\', \\'Singapore\\', \\'South Korea\\', \\'Sri Lanka\\', \\'Syria\\', \\'Tajikistan\\', \\'Thailand\\', \\'Turkmenistan\\', \\'United Arab Emirates\\', \\'Uzbekistan\\', \\'Vietnam\\', \\'Yemen\\', \\'Republic of China\\', \\'United Nations\\', \\'Oceania\\', \\'List of countries\\', \\'Mediterranean Sea\\', \\'Albania\\', \\'Algeria\\', \\'Bosnia and Herzegovina\\', \\'Croatia\\', \\'Cyprus\\', \\'Egypt\\', \\'France\\', \\'Greece\\', \\'Israel\\', \\'Italy\\', \\'Lebanon\\', \\'Libya\\', \\'Malta\\', \\'Monaco\\', \\'Montenegro\\', \\'Morocco\\', \\'Slovenia\\', \\'Spain\\', \\'Syria\\', \\'Tunisia\\', \\'NATO\\', \\'European Union\\', \\'Austria\\', \\'Belgium\\', \\'Cyprus\\', \\'Czech Republic\\', \\'Denmark\\', \\'Estonia\\', \\'Finland\\', \\'France\\', \\'Germany\\', \\'Greece\\', \\'Hungary\\', \\'Republic of Ireland\\', \\'Italy\\', \\'Latvia\\', \\'Lithuania\\', \\'Luxembourg\\', \\'Malta\\', \\'Netherlands\\', \\'Poland\\', \\'Portugal\\', \\'Slovakia\\', \\'Slovenia\\', \\'Spain\\', \\'Sweden\\', \\'United Kingdom\\', \\'Bulgaria\\', \\'Romania\\', \\'Croatia\\', \\'Republic of Macedonia\\', \\'Afghanistan\\', \\'Albania\\', \\'Algeria\\', \\'Azerbaijan\\', \\'Bahrain\\', \\'Bangladesh\\', \\'Benin\\', \\'Burkina Faso\\', \\'Brunei\\', \\'Cameroon\\', \\'Chad\\', \\'Comoros\\', \"Côte d\\'Ivoire\", \\'Djibouti\\', \\'Egypt\\', \\'Gabon\\', \\'Guinea\\', \\'Guinea-Bissau\\', \\'Guyana\\', \\'Indonesia\\', \\'Iran\\', \\'Iraq\\', \\'Jordan\\', \\'Kuwait\\', \\'Kazakhstan\\', \\'Kyrgyzstan\\', \\'Lebanon\\', \\'Libya\\', \\'Maldives\\', \\'Malaysia\\', \\'Mali\\', \\'Mauritania\\', \\'Morocco\\', \\'Mozambique\\', \\'Niger\\', \\'Nigeria\\', \\'Oman\\', \\'Pakistan\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Senegal\\', \\'Sierra Leone\\', \\'Somalia\\', \\'Sudan\\', \\'Syria\\', \\'Tajikistan\\', \\'Tunisia\\', \\'Togo\\', \\'Turkmenistan\\', \\'Uganda\\', \\'Uzbekistan\\', \\'United Arab Emirates\\', \\'Yemen\\', \\'Bosnia and Herzegovina\\', \\'Central African Republic\\', \\'Russia\\', \\'Thailand\\', \\'Turkish Republic of Northern Cyprus\\', \\'United Nations\\', \\'Economy of the Republic of Ireland\\', \\'Azerbaijan\\', \\'Cyprus\\', \\'Turkish Republic of Northern Cyprus\\', \\'Kazakhstan\\', \\'Turkmenistan\\', \\'Uzbekistan\\', \\'Kyrgyzstan\\', \\'Russia\\', \\'Moldova\\', \"People\\'s Republic of China\"]\\n\\n\\n### Response:\\nThe link to reach the target page Oman from the source page Turkey is not present in the provided Input. Therefore, it is not possible to reach the target page in the given number of clicks.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Ottoman_Empire and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Turkey\\', \\'Istanbul\\', \\'List of countries by system of government\\', \\'Monarchy\\', \\'Currency\\', \\'16th century\\', \\'17th century\\', \\'Middle East\\', \\'North Africa\\', \\'Atlantic Ocean\\', \\'Caspian Sea\\', \\'Persian Gulf\\', \\'Austria\\', \\'Slovakia\\', \\'Ukraine\\', \\'Sudan\\', \\'Yemen\\', \\'Istanbul\\', \\'Mediterranean\\', \\'Roman Empire\\', \\'Byzantine Empire\\', \\'Mediterranean Sea\\', \\'Black Sea\\', \\'Red Sea\\', \\'Persian Gulf\\', \\'Indian Ocean\\', \\'Siege\\', \\'Vienna\\', \\'15th century\\', \\'20th century\\', \\'World War I\\', \\'Armenia\\', \\'Byzantine Empire\\', \\'Mediterranean\\', \\'Byzantine Empire\\', \\'Serbia\\', \\'Serbia\\', \\'Sultan\\', \\'Byzantine\\', \\'Suleiman the Magnificent\\', \\'Asia\\', \\'Red Sea\\', \\'Baghdad\\', \\'Mesopotamia\\', \\'Persian Gulf\\', \\'Mediterranean Sea\\', \\'Tunis\\', \\'Algeria\\', \\'Cyprus\\', \\'Spanish Inquisition\\', \\'Holy Roman Empire\\', \\'France\\', \\'France\\', \\'England\\', \\'Habsburg Spain\\', \\'Portugal\\', \\'Persian Gulf\\', \\'Indian Ocean\\', \\'Mediterranean Sea\\', \\'Vienna\\', \\'Istanbul\\', \\'Baghdad\\', \\'Ukraine\\', \\'Poland\\', \\'Holy Roman Empire\\', \\'Austria\\', \\'Egypt\\', \\'Algeria\\', \\'France\\', \\'Johannes Gutenberg\\', \\'Europe\\', \\'Russia\\', \\'Turkey\\', \\'Guild\\', \\'Egypt\\', \\'Cyprus\\', \\'Crimean War\\', \\'Russia\\', \\'Nationalism\\', \\'Serbia\\', \\'Serbia\\', \\'Montenegro\\', \\'Bulgaria\\', \\'Moldova\\', \\'Serbia\\', \\'Romania\\', \\'Montenegro\\', \\'University\\', \\'Constitutional monarchy\\', \\'Nationalism\\', \\'Libya\\', \\'Albania\\', \\'Russia\\', \\'Serbia\\', \\'Bulgaria\\', \\'Greece\\', \\'Bulgaria\\', \\'Montenegro\\', \\'World War I\\', \\'Russian Revolution of 1917\\', \\'Azerbaijan\\', \\'World War I\\', \\'World War I\\', \\'20th century\\', \\'Holocaust\\', \\'France\\', \\'Marco Polo\\', \\'Christopher Columbus\\', \\'Calligraphy\\', \\'Slavery\\', \"Qur\\'an\", \\'Middle Ages\\', \\'Baroque\\', \\'Rococo\\', \\'Istanbul\\', \\'Arabic language\\', \\'Musical instrument\\', \\'Piano\\', \\'Folk music\\', \\'Istanbul\\', \\'European\\', \\'Arabic language\\', \\'Sarajevo\\', \\'Baghdad\\', \\'Beirut\\', \\'Jerusalem\\', \\'Islam\\', \\'Abbasid\\', \\'Greek War of Independence\\', \\'Constitutional monarchy\\', \"Qur\\'an\", \\'Muhammad\\', \\'Mongol Empire\\']\\n\\n\\n### Response:\\nThe link \\'Oman\\' is not present in the provided Input. Therefore, it is not possible to reach the target page in a single click from the source page using the links available in the Input.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Yemen and a target page : Oman, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Arabic language\\', \\'List of countries by system of government\\', \\'Currency\\', \\'Time zone\\', \\'Arabic language\\', \\'Middle East\\', \\'Red Sea\\', \\'Oman\\', \\'Saudi Arabia\\', \\'East Africa\\', \\'Anno Domini\\', \\'Ancient Rome\\', \\'7th century\\', \\'11th century\\', \\'16th century\\', \\'19th century\\', \\'Ottoman Empire\\', \\'Egypt\\', \\'Communist\\', \"Qur\\'an\", \"Qur\\'an\", \"Qur\\'an\", \\'Capital\\', \\'Middle East\\', \\'Red Sea\\', \\'Oman\\', \\'Saudi Arabia\\', \\'Red Sea\\', \\'France\\', \\'Thailand\\', \\'California\\', \\'Malaria\\', \\'Sorghum\\', \\'Cotton\\', \\'Fruit\\', \\'Mango\\', \\'Irrigation\\', \\'Wheat\\', \\'Barley\\', \\'Asia\\', \\'Drought\\', \\'Coffee\\', \\'Indonesia\\', \\'India\\', \\'East Africa\\', \\'United Kingdom\\', \\'United States\\', \\'Soviet Union\\', \\'China\\', \\'Suez Canal\\', \\'Natural gas\\', \\'Iraq\\', \\'Jordan\\', \\'Egypt\\', \\'Marxism\\', \\'Kuwait\\', \\'Eritrea\\', \\'Israel\\', \\'Judaism\\', \\'English language\\', \\'Somalia\\', \\'Eritrea\\', \\'Ethiopia\\', \\'English language\\', \\'Saudi Arabia\\', \\'Oman\\', \\'Eritrea\\', \\'Red Sea\\', \\'Djibouti\\', \\'Somalia\\', \\'List of countries\\', \\'Middle East\\', \\'Bahrain\\', \\'Cyprus\\', \\'Egypt\\', \\'Iran\\', \\'Iraq\\', \\'Israel\\', \\'Jordan\\', \\'Kuwait\\', \\'Lebanon\\', \\'Oman\\', \\'Palestinian territories\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Syria\\', \\'Turkey\\', \\'United Arab Emirates\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Bahrain\\', \\'Cyprus\\', \\'Georgia (country)\\', \\'Iran\\', \\'Iraq\\', \\'Israel\\', \\'Jordan\\', \\'Kuwait\\', \\'Lebanon\\', \\'Oman\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Syria\\', \\'Turkey\\', \\'United Arab Emirates\\', \\'List of countries\\', \\'Asia\\', \\'Afghanistan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Bahrain\\', \\'Bangladesh\\', \\'Bhutan\\', \\'Brunei\\', \\'Cambodia\\', \"People\\'s Republic of China\", \\'Cyprus\\', \\'East Timor\\', \\'Georgia (country)\\', \\'India\\', \\'Indonesia\\', \\'Iran\\', \\'Iraq\\', \\'Israel\\', \\'Japan\\', \\'Jordan\\', \\'Kazakhstan\\', \\'Kuwait\\', \\'Kyrgyzstan\\', \\'Laos\\', \\'Lebanon\\', \\'Malaysia\\', \\'Maldives\\', \\'Mongolia\\', \\'Myanmar\\', \\'Nepal\\', \\'North Korea\\', \\'Oman\\', \\'Pakistan\\', \\'Philippines\\', \\'Qatar\\', \\'Russia\\', \\'Saudi Arabia\\', \\'Singapore\\', \\'South Korea\\', \\'Sri Lanka\\', \\'Syria\\', \\'Tajikistan\\', \\'Thailand\\', \\'Turkey\\', \\'Turkmenistan\\', \\'United Arab Emirates\\', \\'Uzbekistan\\', \\'Vietnam\\', \\'Republic of China\\', \\'United Nations\\', \\'Oceania\\', \\'List of countries\\', \\'India\\', \\'Iran\\', \\'Maldives\\', \\'Oman\\', \\'Pakistan\\', \\'Somalia\\', \\'Sri Lanka\\', \\'List of countries\\', \\'Red Sea\\', \\'Djibouti\\', \\'Egypt\\', \\'Eritrea\\', \\'Israel\\', \\'Jordan\\', \\'Saudi Arabia\\', \\'Somalia\\', \\'Sudan\\', \\'List of countries\\', \\'Indian Ocean\\', \\'Bahrain\\', \\'Bangladesh\\', \\'Christmas Island\\', \\'Cocos (Keeling) Islands\\', \\'India\\', \\'Indonesia\\', \\'Iran\\', \\'Iraq\\', \\'Israel\\', \\'Kuwait\\', \\'Malaysia\\', \\'Maldives\\', \\'Myanmar\\', \\'Oman\\', \\'Pakistan\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Sri Lanka\\', \\'Thailand\\', \\'United Arab Emirates\\', \\'Africa\\', \\'Comoros\\', \\'Djibouti\\', \\'Eritrea\\', \\'Kenya\\', \\'Madagascar\\', \\'Mauritius\\', \\'Mayotte\\', \\'Mozambique\\', \\'Seychelles\\', \\'Somalia\\', \\'South Africa\\', \\'Sudan\\', \\'Tanzania\\', \\'Oceania\\', \\'Australia\\', \\'Christmas Island\\', \\'Cocos (Keeling) Islands\\', \\'Bahrain\\', \\'Christmas Island\\', \\'Cocos (Keeling) Islands\\', \\'Madagascar\\', \\'Maldives\\', \\'Mauritius\\', \\'Mayotte\\', \\'Seychelles\\', \\'Sri Lanka\\', \\'Arab League\\', \\'Algeria\\', \\'Bahrain\\', \\'Comoros\\', \\'Djibouti\\', \\'Egypt\\', \\'Iraq\\', \\'Jordan\\', \\'Kuwait\\', \\'Lebanon\\', \\'Libya\\', \\'Mauritania\\', \\'Morocco\\', \\'Oman\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Somalia\\', \\'Sudan\\', \\'Syria\\', \\'Tunisia\\', \\'United Arab Emirates\\', \\'Afghanistan\\', \\'Albania\\', \\'Algeria\\', \\'Azerbaijan\\', \\'Bahrain\\', \\'Bangladesh\\', \\'Benin\\', \\'Burkina Faso\\', \\'Brunei\\', \\'Cameroon\\', \\'Chad\\', \\'Comoros\\', \"Côte d\\'Ivoire\", \\'Djibouti\\', \\'Egypt\\', \\'Gabon\\', \\'Guinea\\', \\'Guinea-Bissau\\', \\'Guyana\\', \\'Indonesia\\', \\'Iran\\', \\'Iraq\\', \\'Jordan\\', \\'Kuwait\\', \\'Kazakhstan\\', \\'Kyrgyzstan\\', \\'Lebanon\\', \\'Libya\\', \\'Maldives\\', \\'Malaysia\\', \\'Mali\\', \\'Mauritania\\', \\'Morocco\\', \\'Mozambique\\', \\'Niger\\', \\'Nigeria\\', \\'Oman\\', \\'Pakistan\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Senegal\\', \\'Sierra Leone\\', \\'Somalia\\', \\'Sudan\\', \\'Syria\\', \\'Tajikistan\\', \\'Turkey\\', \\'Tunisia\\', \\'Togo\\', \\'Turkmenistan\\', \\'Uganda\\', \\'Uzbekistan\\', \\'United Arab Emirates\\', \\'Bosnia and Herzegovina\\', \\'Central African Republic\\', \\'Russia\\', \\'Thailand\\', \\'Turkish Republic of Northern Cyprus\\', \\'United Nations\\', \\'Arabic language\\', \\'Algeria\\', \\'Bahrain\\', \\'Egypt\\', \\'Iraq\\', \\'Jordan\\', \\'Kuwait\\', \\'Lebanon\\', \\'Libya\\', \\'Malta\\', \\'Mauritania\\', \\'Morocco\\', \\'Oman\\', \\'Qatar\\', \\'Saudi Arabia\\', \\'Sudan\\', \\'Syria\\', \\'Tunisia\\', \\'United Arab Emirates\\', \\'Western Sahara\\', \\'Hebrew language\\', \\'Iraq\\', \\'Israel\\', \\'Syria\\', \\'Eritrea\\', \\'Ethiopia\\', \\'Oman\\']\\n\\n\\n### Response:\\nThe link to reach the target page Oman from the source page Yemen is [\\'Oman\\'].\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n",
      "The value could not be extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Iron(III)_chloride and a target page : Philosophy, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Phase (matter)\\', \\'Acetone\\', \\'Metal\\', \\'Aluminium chloride\\', \\'Chlorine\\', \\'Hydrochloric acid\\', \\'Ethanol\\', \\'Ammonia\\', \\'Copper(I) chloride\\', \\'Iron\\', \\'Chlorine\\', \\'Gas\\', \\'Iron\\', \\'Hydrochloric acid\\', \\'Chlorine\\', \\'Copper\\', \\'Copper(I) chloride\\', \\'Copper(I) chloride\\', \\'Chlorine\\', \\'Aluminium chloride\\', \\'Sodium hydroxide\\', \\'Ethanol\\']\\n\\n\\n### Response:\\nThe target page, Philosophy, cannot be reached directly from the given source page, Iron(III)_chloride, in a single click using the provided links. Therefore, no link should be chosen from the Input.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Phase_(matter) and a target page : Philosophy, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Physical science\\', \\'Set\\', \\'Gas\\', \\'Plasma (physics)\\', \\'Liquid crystal\\', \\'Magnet\\', \\'Thermodynamics\\', \\'Thermodynamics\\', \\'Entropy\\', \\'Entropy\\', \\'Glass\\', \\'Crystal\\', \\'Liquid crystal\\', \\'Ice\\', \\'Diamond\\', \\'Carbon\\', \\'Electron\\', \\'Plasma (physics)\\', \\'Absolute zero\\', \\'Metal\\', \\'Semiconductor\\', \\'Star\\', \\'Big Bang\\', \\'Photon\\', \\'Water\\', \\'Diamond\\', \\'Carbon\\', \\'Steel\\', \\'Iron\\', \\'Ice\\']\\n\\n\\n### Response:\\nThe link \\'Philosophy\\' is not present in the provided Input, so it is not possible to reach the target page in a single click from the source page using the links in the Input.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Physical_science and a target page : Philosophy, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Science\\', \\'Biology\\', \\'Astronomy\\', \\'Chemistry\\', \\'Geology\\', \\'Hydrology\\', \\'Meteorology\\', \\'Oceanography\\', \\'Soil science\\', \\'Physics\\', \\'Science\\', \\'Astronomy\\', \\'Science\\', \\'Stars\\', \\'Galaxy\\', \\'Big Bang\\', \\'Universe\\', \\'Nicolaus Copernicus\\', \\'Heliocentrism\\', \\'Solar system\\', \\'Asteroid\\', \\'Earth\\', \\'Moon\\', \\'Astrology\\', \\'Chemistry\\', \\'Science\\', \\'Matter\\', \\'Quantum mechanics\\', \\'Chemical element\\', \\'Dmitri Mendeleev\\', \\'Chemical element\\', \\'Water\\', \\'Chemical element\\', \\'Science\\', \\'Earth (planet)\\', \\'Life\\', \\'Oceanography\\', \\'Geology\\', \\'Soil science\\', \\'Meteorology\\', \\'Weather\\', \\'Climate\\', \\'Hydrology\\', \\'Thunderstorm\\', \\'Tornado\\', \\'Hurricane\\', \\'Weather forecasting\\', \\'Physics\\', \\'Biology\\', \\'Chemistry\\', \\'Geology\\', \\'Matter\\', \\'Energy\\', \\'Motion (physics)\\', \\'Force\\', \\'Momentum\\', \\'Gravity\\', \\'Energy\\', \\'Mechanical work\\', \\'Motion (physics)\\', \\'Energy\\', \\'Phase (matter)\\', \\'Energy\\', \\'Heat\\', \\'Waves\\', \\'Sound\\', \\'Electricity\\', \\'Magnetism\\', \\'Light\\', \\'Aristotle\\', \\'Ancient Greece\\', \\'Empiricism\\', \\'Archimedes\\', \\'Robert Boyle\\', \\'Irish people\\', \\'Chemistry\\', \\'Alchemy\\', \\'Chemical element\\', \\'Nicolaus Copernicus\\', \\'Sun\\', \\'Solar system\\', \\'Marie Curie\\', \\'France\\', \\'Polonium\\', \\'Radium\\', \\'Albert Einstein\\', \\'Leonhard Euler\\', \\'Switzerland\\', \\'Science\\', \\'Roman Catholic Church\\', \\'Science\\', \\'Astronomy\\', \\'Philosopher\\', \\'Isaac Newton\\', \\'Linus Pauling\\', \\'United States\\', \\'Quantum chemistry\\', \\'Quantum mechanics\\', \\'Ancient Greece\\', \\'Philosopher\\', \\'Science\\']\\n\\n\\n### Response:\\nThe link \\'Philosopher\\' is the closest link to the target page \\'Philosophy\\'.</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : Science and a target page : Philosophy, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'Society\\', \\'Mathematics\\', \\'Atom\\', \\'History of science\\', \\'Latin\\', \\'Sanskrit\\', \\'Middle Ages\\', \\'Age of Enlightenment\\', \\'Philosophy\\', \\'Physical science\\', \\'Ethics\\', \\'Nature\\', \\'Universe\\', \\'Quantum mechanics\\', \\'Mathematics\\', \\'Calculus\\', \\'Statistics\\', \\'Topology\\', \\'Physics\\', \\'Chemistry\\', \\'Richard Feynman\\', \\'Bertrand Russell\\', \\'John Locke\\', \\'Chemistry\\', \\'Biology\\', \\'Ethics\\', \\'Pollution\\', \\'University\\', \\'Technology\\', \\'Tobacco smoking\\', \\'Manufacturing\\', \\'Technology\\', \\'Latin\\', \\'World Wide Web\\', \\'Engineering\\', \\'Linguistics\\', \\'Archaeology\\', \\'Paleontology\\', \\'Computer science\\', \\'Political science\\', \\'Karl Popper\\', \\'Thomas Samuel Kuhn\\', \\'Renaissance\\', \\'Italy\\', \\'United States\\']\\n\\n\\n### Response:\\nThe link \\'Philosophy\\' is the one that should be chosen to reach the target page in as few clicks as possible.</s>']\n",
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven two Wikipedia pages, a source page : 15th_century and a target page : Anglican_Church_of_Australia, your goal is to reach the target page starting from the source page in as few clicks as possible. \\n\\n\\n### Constraint:\\nYou are provided the links available on the source page in the Input and should choose exclusively one link among those proposed links.\"\\n\\n\\n### Input:\\n[\\'14th century\\', \\'16th century\\', \\'Time\\', \\'Timur\\', \\'Baghdad\\', \\'Ottoman Empire\\', \\'Beijing\\', \\'Zheng He\\', \\'Indian Ocean\\', \\'Battle of Grunwald\\', \\'France\\', \\'Joan of Arc\\', \"Hundred Years\\' War\", \\'Inca Empire\\', \\'Aztec\\', \\'Byzantine Empire\\', \"Hundred Years\\' War\", \\'Wars of the Roses\\', \\'Spain\\', \\'France\\', \\'Spanish Inquisition\\', \\'Jews\\', \\'Spain\\', \\'Christopher Columbus\\', \\'Vasco da Gama\\', \\'Europe\\', \\'India\\', \\'Zheng He\\', \\'Henry V of England\\', \\'Johannes Ockeghem\\', \\'Joan of Arc\\', \\'France\\', \\'Poland\\', \\'Lithuania\\', \\'Ottoman Empire\\', \\'Ottoman Empire\\', \\'Plato\\', \\'Christopher Columbus\\', \\'Spain\\', \\'Richard III of England\\', \\'Leonardo da Vinci\\', \\'Henry VII of England\\', \\'Vasco da Gama\\', \\'Portugal\\', \\'Renaissance\\', \\'Philosophy\\', \\'Art\\', \\'Korea\\', \\'Korea\\', \\'North America\\', \\'Christopher Columbus\\', \\'1st century BC\\', \\'1st century\\', \\'2nd century\\', \\'3rd century\\', \\'4th century\\', \\'5th century\\', \\'6th century\\', \\'7th century\\', \\'8th century\\', \\'9th century\\', \\'10th century\\', \\'11th century\\', \\'12th century\\', \\'13th century\\', \\'14th century\\', \\'16th century\\', \\'17th century\\', \\'18th century\\', \\'19th century\\', \\'20th century\\', \\'21st century\\']\\n\\n\\n### Response:\\nThe Anglican Church of Australia is not directly linked from the 15th century page. Therefore, it is not possible to reach the target page in a single click from the provided links.\\n\\n\\n### Explanation:\\nThe input is a list of strings representing the links available on the source page']\n",
      "The value could not be extracted.\n"
     ]
    }
   ],
   "source": [
    "model_preds = []\n",
    "for i in range(20) :\n",
    "    input = eval_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "        test_dataset[i]['instruction'], # instruction\n",
    "          test_dataset[i]['input'], # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = ft_model.generate(**input, max_new_tokens = 64, use_cache = True)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)\n",
    "    print(decoded_output)\n",
    "    final_output = retrieve_output(decoded_output)\n",
    "    model_preds.append(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdfe92",
   "metadata": {},
   "source": [
    "### Import Data links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37afd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name_links = load_dataset(\"berquetR/file_names_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c959e5",
   "metadata": {},
   "source": [
    "### Testing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad1e0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_from_source(dataset, source):\n",
    "    filtered = page_name_links['train'].filter(lambda example : example['filename'] == source)\n",
    "\n",
    "    if len(filtered) > 0:\n",
    "        row_with_filename_e = filtered[0]\n",
    "        links = row_with_filename_e['links']\n",
    "        return links\n",
    "    else:\n",
    "        print(\"No rows found where filename = 'e'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "023619d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def retrieve_output (model_output) :\n",
    "    match = re.search(r\"### Response:\\n(.+?)</s>\", model_output[0])\n",
    "\n",
    "    if match:\n",
    "        response_value = str(match.group(1).lstrip())\n",
    "        return response_value\n",
    "    else:\n",
    "        print(\"The value could not be extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d68bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(source_links, source, original_source, target):\n",
    "    instruction = f\"We are currently on the Wikipedia page: {source}, we originally started on the Wikipedia page: {original_source}, given the following Wikipedia pages links, on which one should we click on to arrive the fastest to the Wikipedia page in terms of visited pages : {target}.\"\n",
    "\n",
    "    input = source_links\n",
    "    output = ' '\n",
    "\n",
    "    return {'instruction': instruction, 'input': input, 'output': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "244cbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = 'Nationalism'\n",
    "original_source = 'Nationalism'\n",
    "source_links = '[\\'Human\\', \\'Politics\\', \\'History of the world\\', \\'Denmark\\', \\'Culture\\', \\'Music\\', \\'Literature\\', \\'Folklore\\', \\'Mythology\\', \\'Religion\\', \\'Cultural identity\\', \\'Language\\', \\'Culture\\', \\'Ethnic group\\', \\'Gender\\', \\'Education\\', \\'Ottoman Empire\\', \\'Russia\\', \\'China\\', \\'Vatican City\\', \\'Roman Catholic Church\\', \\'Ethics\\', \\'War\\', \\'Marxism\\', \\'Socialism\\', \\'Sociology\\', \\'World War II\\', \\'French Revolution\\', \\'Napoleon\\', \\'Capitalism\\', \\'Mythology\\', \\'French Revolution\\', \\'Liberalism\\', \\'Ottoman Empire\\', \\'Spain\\', \\'South America\\', \\'Netherlands\\', \\'England\\', \\'Asia\\', \\'India\\', \\'Mahatma Gandhi\\', \\'China\\', \\'Japan\\', \\'World War I\\', \\'Ottoman Empire\\', \\'Woodrow Wilson\\', \\'Iraq\\', \\'Lebanon\\', \\'Syria\\', \\'Russian Revolution of 1917\\', \\'Ireland\\', \\'Northern Ireland\\', \\'Fascism\\', \\'Nazism\\', \\'Italy\\', \\'Germany\\', \\'World War II\\', \\'The Holocaust\\', \\'Africa\\', \\'Somalia\\', \\'Portugal\\', \\'Mozambique\\', \\'Angola\\', \\'Belarus\\', \\'Ukraine\\', \\'Moldova\\', \\'Kazakhstan\\', \\'Turkmenistan\\', \\'Uzbekistan\\', \\'Tajikistan\\', \\'Kyrgyzstan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Georgia (country)\\', \\'Latvia\\', \\'Estonia\\', \\'Lithuania\\', \\'Yugoslavia\\', \\'Northern Ireland\\', \\'European Union\\', \\'Globalization\\', \"People\\'s Republic of China\", \\'European Union\\', \\'French Revolution\\', \\'19th century\\', \\'Literacy\\', \\'Newspaper\\', \\'Book\\', \\'Ireland\\', \\'India\\', \\'Hebrew language\\', \\'United States\\', \\'German language\\', \\'World War I\\', \\'French language\\', \\'French language\\', \\'Spanish language\\', \\'English language\\', \\'Arabic language\\', \\'Algeria\\', \\'Western Sahara\\', \\'Nobiin language\\', \\'Egypt\\', \\'Sudan\\', \\'Morocco\\', \\'Civil society\\', \\'Citizenship\\', \\'Jean-Jacques Rousseau\\', \\'Liberalism\\', \\'Race\\', \\'Romanticism\\', \\'Folklore\\', \\'Brothers Grimm\\', \\'Culture\\', \\'Taiwan\\', \\'Fascism\\', \\'Liberalism\\', \\'Religion\\', \\'Ireland\\', \\'18th century\\', \\'Northern Ireland\\', \\'India\\', \\'Hinduism\\', \\'Zionism\\', \\'World War I\\', \\'Treaty of Versailles\\', \\'Lithuania\\', \\'Fascism\\', \\'Socialism\\', \\'Nazism\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'Nazism\\', \\'Race\\', \\'Nazi Germany\\', \\'Ottoman Empire\\', \\'Liberalism\\', \\'War\\', \\'Germany\\', \\'Marxism\\', \\'World War I\\', \\'Politics\\', \\'Anarchism\\', \\'Race\\', \\'Islam\\', \\'Government\\', \\'Islam\\', \\'United Nations\\']'\n",
    "target = 'Antwerp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "68f22d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWe are currently on the Wikipedia page: Nationalism, we originally started on the Wikipedia page: Nationalism, given the following Wikipedia pages links, on which one should we click on to arrive the fastest to the Wikipedia page in terms of visited pages : Antwerp.\\n\\n### Input:\\n[\\'Human\\', \\'Politics\\', \\'History of the world\\', \\'Denmark\\', \\'Culture\\', \\'Music\\', \\'Literature\\', \\'Folklore\\', \\'Mythology\\', \\'Religion\\', \\'Cultural identity\\', \\'Language\\', \\'Culture\\', \\'Ethnic group\\', \\'Gender\\', \\'Education\\', \\'Ottoman Empire\\', \\'Russia\\', \\'China\\', \\'Vatican City\\', \\'Roman Catholic Church\\', \\'Ethics\\', \\'War\\', \\'Marxism\\', \\'Socialism\\', \\'Sociology\\', \\'World War II\\', \\'French Revolution\\', \\'Napoleon\\', \\'Capitalism\\', \\'Mythology\\', \\'French Revolution\\', \\'Liberalism\\', \\'Ottoman Empire\\', \\'Spain\\', \\'South America\\', \\'Netherlands\\', \\'England\\', \\'Asia\\', \\'India\\', \\'Mahatma Gandhi\\', \\'China\\', \\'Japan\\', \\'World War I\\', \\'Ottoman Empire\\', \\'Woodrow Wilson\\', \\'Iraq\\', \\'Lebanon\\', \\'Syria\\', \\'Russian Revolution of 1917\\', \\'Ireland\\', \\'Northern Ireland\\', \\'Fascism\\', \\'Nazism\\', \\'Italy\\', \\'Germany\\', \\'World War II\\', \\'The Holocaust\\', \\'Africa\\', \\'Somalia\\', \\'Portugal\\', \\'Mozambique\\', \\'Angola\\', \\'Belarus\\', \\'Ukraine\\', \\'Moldova\\', \\'Kazakhstan\\', \\'Turkmenistan\\', \\'Uzbekistan\\', \\'Tajikistan\\', \\'Kyrgyzstan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Georgia (country)\\', \\'Latvia\\', \\'Estonia\\', \\'Lithuania\\', \\'Yugoslavia\\', \\'Northern Ireland\\', \\'European Union\\', \\'Globalization\\', \"People\\'s Republic of China\", \\'European Union\\', \\'French Revolution\\', \\'19th century\\', \\'Literacy\\', \\'Newspaper\\', \\'Book\\', \\'Ireland\\', \\'India\\', \\'Hebrew language\\', \\'United States\\', \\'German language\\', \\'World War I\\', \\'French language\\', \\'French language\\', \\'Spanish language\\', \\'English language\\', \\'Arabic language\\', \\'Algeria\\', \\'Western Sahara\\', \\'Nobiin language\\', \\'Egypt\\', \\'Sudan\\', \\'Morocco\\', \\'Civil society\\', \\'Citizenship\\', \\'Jean-Jacques Rousseau\\', \\'Liberalism\\', \\'Race\\', \\'Romanticism\\', \\'Folklore\\', \\'Brothers Grimm\\', \\'Culture\\', \\'Taiwan\\', \\'Fascism\\', \\'Liberalism\\', \\'Religion\\', \\'Ireland\\', \\'18th century\\', \\'Northern Ireland\\', \\'India\\', \\'Hinduism\\', \\'Zionism\\', \\'World War I\\', \\'Treaty of Versailles\\', \\'Lithuania\\', \\'Fascism\\', \\'Socialism\\', \\'Nazism\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'Nazism\\', \\'Race\\', \\'Nazi Germany\\', \\'Ottoman Empire\\', \\'Liberalism\\', \\'War\\', \\'Germany\\', \\'Marxism\\', \\'World War I\\', \\'Politics\\', \\'Anarchism\\', \\'Race\\', \\'Islam\\', \\'Government\\', \\'Islam\\', \\'United Nations\\']\\n\\n### Response:\\nEuropean_Union</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWe are currently on the Wikipedia page: European_Union, we originally started on the Wikipedia page: Nationalism, given the following Wikipedia pages links, on which one should we click on to arrive the fastest to the Wikipedia page in terms of visited pages : Antwerp.\\n\\n### Input:\\n['Brussels', 'Dutch language', 'English language', 'French language', 'German language', 'Portuguese language', 'Spanish language', 'Austria', 'Belgium', 'Bulgaria', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Republic of Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'United Kingdom', 'Government', 'European Parliament', 'Germany', 'Water', 'Currency', 'Currency', 'Euro', 'Time zone', 'Euro', 'European Parliament', 'Europe', 'Soviet Union', 'Winston Churchill', 'Germany', 'France', 'Italy', 'Mediterranean Sea', 'Greece', 'Switzerland', 'Norway', 'Russia', 'French Guiana', 'Martinique', 'Guadeloupe', 'Greenland', 'Faroe Islands', 'Aruba', 'Netherlands Antilles', 'New Caledonia', 'Austria', 'Poland', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Republic of Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Republic of Macedonia', 'Malta', 'Netherlands', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Turkey', 'United Kingdom', 'Belgium', 'France', 'Italy', 'Luxembourg', 'Netherlands', 'Denmark', 'Republic of Ireland', 'United Kingdom', 'Greece', 'Portugal', 'Spain', 'Austria', 'Finland', 'Sweden', 'Cyprus', 'Czech Republic', 'Estonia', 'Hungary', 'Latvia', 'Lithuania', 'Malta', 'Poland', 'Slovakia', 'Slovenia', 'Bulgaria', 'Romania', 'Greenland', 'Denmark', 'Channel Islands', 'Faroe Islands', 'Martinique', 'French Guiana', 'Peace', 'Democracy', 'World War II', 'United States', 'Soviet Union', 'European', 'Tony Blair', 'Margaret Thatcher', 'European Parliament', 'Euro', 'Poland', 'European Parliament', 'European Parliament', 'Condoleezza Rice', 'NATO', 'Crime', 'Law', 'United Nations', 'World Trade Organization', 'United States', 'NATO', 'Bulgaria', 'Romania', 'Copenhagen', 'European Parliament', 'Cyprus', 'Croatia', 'Albania', 'Bosnia and Herzegovina', 'Montenegro', 'Serbia', 'Norway', 'Switzerland', 'Iceland', 'European Parliament', 'European Parliament', 'Frankfurt', 'Germany', 'European Parliament', 'European Parliament', 'Luxembourg (city)', 'Finland', 'Denmark', 'Infrastructure', 'London', 'Petroleum', 'Natural gas', 'Waste management', 'Recycling', 'Carbon dioxide', 'Biofuel', 'Global warming', 'Global city', 'Athens', 'Cologne', 'Düsseldorf', 'Amsterdam', 'Rotterdam', 'The Hague', 'Frankfurt', 'London', 'Paris', 'London', 'Berlin', 'London', 'Paris', 'Madrid', 'Madrid', 'Madrid', 'Rome', 'Barcelona', 'Berlin', 'London', 'Paris', 'Barcelona', 'Berlin', 'Bucharest', 'Milan', 'Barcelona', 'Hamburg', 'Berlin', 'Milan', 'Warsaw', 'Rotterdam', 'Athens', 'Budapest', 'Athens', 'Rome', 'Vienna', 'Hamburg', 'Madrid', 'Milan', 'Paris', 'Latin alphabet', 'Greece', 'Cyprus', 'Bulgaria', 'Dutch language', 'English language', 'French language', 'German language', 'Portuguese language', 'Spanish language', 'Latin alphabet', 'Russian language', 'Hindi', 'Eastern Orthodox Church', 'Czech Republic', 'Estonia', 'Christianity', 'Atheism', 'Agnosticism', 'Judaism', 'Islam', 'Buddhism', 'Hinduism', 'Sikhism', 'Sweden', 'Norway', 'Iceland', 'Liechtenstein', 'Turkey', 'Europe', 'Bologna', 'Prague', 'Berlin', 'London', 'European Space Agency', 'Switzerland', 'Norway', 'Romania', 'Ancient Greece', 'Ancient Rome', 'Middle Ages', 'Renaissance', 'Age of Enlightenment', 'Liberalism', 'Christianity', 'Football (soccer)', 'Luxembourg', 'Football (soccer)', 'Barcelona', 'Spain', 'Football (soccer)', 'Tennis', 'Ice hockey', 'Rugby football', 'Cricket']\\n\\n### Response:\\nBelgium</s>\"]\n",
      "[\"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWe are currently on the Wikipedia page: Belgium, we originally started on the Wikipedia page: Nationalism, given the following Wikipedia pages links, on which one should we click on to arrive the fastest to the Wikipedia page in terms of visited pages : Antwerp.\\n\\n### Input:\\n['Dutch language', 'French language', 'German language', 'English language', 'Brussels', 'Antwerp', 'Dutch language', 'French language', 'German language', 'List of countries by system of government', 'European Union', 'Currency', 'Euro', 'Time zone', 'European Union', 'Dutch language', 'French language', 'German language', 'Netherlands', 'Germany', 'Luxembourg', 'France', 'European Union', 'Dutch language', 'French language', 'Latin', 'Middle Ages', 'European Union', 'NATO', 'Anno Domini', 'Franks', 'Habsburg Spain', 'Constitutional monarchy', 'World War II', 'Oligarchy', 'Trade union', 'Rwanda', 'Burundi', 'League of Nations', 'Blitzkrieg', 'World War II', 'European Union', 'European Parliament', 'Constitutional monarchy', 'Parliamentary system', 'Nationalism', 'Trade union', 'Liberalism', 'Dutch language', 'Flanders', 'Brussels', 'Antwerp', 'Temperate', 'Flanders', 'Brussels', 'Industrial Revolution', 'Antwerp', 'Petroleum', '1973 oil crisis', 'Euro', 'Luxembourg', 'United Nations', 'Brussels', 'Roman Catholic Church', 'Monaco', 'Antwerp', 'Dutch language', 'Dutch language', 'Flemish people', 'Italy', 'Morocco', 'France', 'Netherlands', 'Vienna', 'Renaissance', 'Baroque', 'Peter Paul Rubens', 'Anthony van Dyck', 'Saxophone', 'Romanticism', 'Expressionism', 'The Adventures of Tintin', 'Mathematics', 'Anatomy', 'Engineering', 'Big Bang', 'Tour de France', 'Lettuce', 'United Kingdom', 'North Sea', 'Netherlands', 'Germany', 'France', 'Luxembourg']\\n\\n### Response:\\nAntwerp</s>\"]\n"
     ]
    }
   ],
   "source": [
    "while target != model_choice :\n",
    "\n",
    "    source = model_choice\n",
    "    source_links = links_from_source(page_name_links , source)\n",
    "    prompt =  get_prompt(source_links, source, original_source, target)\n",
    "    input = eval_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "        prompt['instruction'], # instruction\n",
    "          prompt['input'], # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = ft_model.generate(**input, max_new_tokens = 64, use_cache = True)\n",
    "    decoded_output = eval_tokenizer.batch_decode(outputs)\n",
    "    print(decoded_output)\n",
    "    model_choice = retrieve_output(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "143fe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "87a1e58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Nationalism',\n",
       " 'target': 'Antwerp',\n",
       " 'current_page': 'Nationalism',\n",
       " 'current_page_links': '[\\'Human\\', \\'Politics\\', \\'History of the world\\', \\'Denmark\\', \\'Culture\\', \\'Music\\', \\'Literature\\', \\'Folklore\\', \\'Mythology\\', \\'Religion\\', \\'Cultural identity\\', \\'Language\\', \\'Culture\\', \\'Ethnic group\\', \\'Gender\\', \\'Education\\', \\'Ottoman Empire\\', \\'Russia\\', \\'China\\', \\'Vatican City\\', \\'Roman Catholic Church\\', \\'Ethics\\', \\'War\\', \\'Marxism\\', \\'Socialism\\', \\'Sociology\\', \\'World War II\\', \\'French Revolution\\', \\'Napoleon\\', \\'Capitalism\\', \\'Mythology\\', \\'French Revolution\\', \\'Liberalism\\', \\'Ottoman Empire\\', \\'Spain\\', \\'South America\\', \\'Netherlands\\', \\'England\\', \\'Asia\\', \\'India\\', \\'Mahatma Gandhi\\', \\'China\\', \\'Japan\\', \\'World War I\\', \\'Ottoman Empire\\', \\'Woodrow Wilson\\', \\'Iraq\\', \\'Lebanon\\', \\'Syria\\', \\'Russian Revolution of 1917\\', \\'Ireland\\', \\'Northern Ireland\\', \\'Fascism\\', \\'Nazism\\', \\'Italy\\', \\'Germany\\', \\'World War II\\', \\'The Holocaust\\', \\'Africa\\', \\'Somalia\\', \\'Portugal\\', \\'Mozambique\\', \\'Angola\\', \\'Belarus\\', \\'Ukraine\\', \\'Moldova\\', \\'Kazakhstan\\', \\'Turkmenistan\\', \\'Uzbekistan\\', \\'Tajikistan\\', \\'Kyrgyzstan\\', \\'Armenia\\', \\'Azerbaijan\\', \\'Georgia (country)\\', \\'Latvia\\', \\'Estonia\\', \\'Lithuania\\', \\'Yugoslavia\\', \\'Northern Ireland\\', \\'European Union\\', \\'Globalization\\', \"People\\'s Republic of China\", \\'European Union\\', \\'French Revolution\\', \\'19th century\\', \\'Literacy\\', \\'Newspaper\\', \\'Book\\', \\'Ireland\\', \\'India\\', \\'Hebrew language\\', \\'United States\\', \\'German language\\', \\'World War I\\', \\'French language\\', \\'French language\\', \\'Spanish language\\', \\'English language\\', \\'Arabic language\\', \\'Algeria\\', \\'Western Sahara\\', \\'Nobiin language\\', \\'Egypt\\', \\'Sudan\\', \\'Morocco\\', \\'Civil society\\', \\'Citizenship\\', \\'Jean-Jacques Rousseau\\', \\'Liberalism\\', \\'Race\\', \\'Romanticism\\', \\'Folklore\\', \\'Brothers Grimm\\', \\'Culture\\', \\'Taiwan\\', \\'Fascism\\', \\'Liberalism\\', \\'Religion\\', \\'Ireland\\', \\'18th century\\', \\'Northern Ireland\\', \\'India\\', \\'Hinduism\\', \\'Zionism\\', \\'World War I\\', \\'Treaty of Versailles\\', \\'Lithuania\\', \\'Fascism\\', \\'Socialism\\', \\'Nazism\\', \\'Adolf Hitler\\', \\'Nazi Germany\\', \\'Nazism\\', \\'Race\\', \\'Nazi Germany\\', \\'Ottoman Empire\\', \\'Liberalism\\', \\'War\\', \\'Germany\\', \\'Marxism\\', \\'World War I\\', \\'Politics\\', \\'Anarchism\\', \\'Race\\', \\'Islam\\', \\'Government\\', \\'Islam\\', \\'United Nations\\']',\n",
       " 'next_page': 'World_War_II',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0329221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env2)",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
