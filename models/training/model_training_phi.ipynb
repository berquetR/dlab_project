{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4601088",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcda581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6318e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a74db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3f1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486f0b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-40GB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f3d0f",
   "metadata": {},
   "source": [
    "### Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9b77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"berquetR/dlab_project_optimal_links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85732370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed124de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'],\n",
       "    num_rows: 78088\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c487ac34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'],\n",
       "    num_rows: 26178\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50313820",
   "metadata": {},
   "source": [
    "### Build Prompt Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1e3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format a row according to your fine-tuning requirements\n",
    "def format_row(row):\n",
    "    input_data = {\n",
    "        \"Source\": row['current_page'], \n",
    "        \"Candidates\": row['current_page_links'], \n",
    "        \"Target\": row['target']\n",
    "    }\n",
    "    output_data = {\n",
    "        \"Output\": row['next_page']\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"You are a knowledge discovery expert familiar with the Wikipedia link structure and your objective is to play the game of Wikispeedia: https://dlab.epfl.ch/wikispeedia/play/.\n",
    "##Goal \n",
    "Given two Wikipedia articles, a source and a target, your goal is to reach the target article starting from the source article in as few clicks as possible. For the articles you are given this is always possible.\n",
    "\n",
    "##Constraint \n",
    "You should exclusively follow the links present in the articles that you encounter along the way.\n",
    "\n",
    "##Fine-grained instructions \n",
    "1. While the overall goal is to find a path from a source to a target article, you will proceed step by step.\n",
    "2. Given outgoing links from the source article as candidates, you should select the candidate that takes you closer to the target article. Use your knowledge of the \"expected\" Wikipedia link structure and relatedness between articles to identify the candidate that takes you closer to the target.\n",
    "3. Choose **only** from the provided candidates.\n",
    "4. Do not provide an algorithm or code to solve the task, instead give only the solution.\n",
    "\n",
    "##Input \n",
    "{json.dumps(input_data, indent=4)}\n",
    "\n",
    "##Output \n",
    "{json.dumps(output_data, indent=4)}\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6137b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to each row\n",
    "train_dataset = train_dataset.map(lambda x: {\"text\": format_row(x)})\n",
    "\n",
    "# You might want to remove the old columns and keep only 'text'\n",
    "train_dataset = train_dataset.remove_columns(['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d6c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to each row\n",
    "validation_dataset = validation_dataset.map(lambda x: {\"text\": format_row(x)})\n",
    "\n",
    "# You might want to remove the old columns and keep only 'text'\n",
    "validation_dataset = validation_dataset.remove_columns(['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777c215",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ef0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f51b6",
   "metadata": {},
   "source": [
    "### Model import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ceefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba66c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-1_5\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4cbe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"microsoft/phi-1_5\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2bcd4",
   "metadata": {},
   "source": [
    "### Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1bdb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d2bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03d5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d721e95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8630299",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './phi_1_5_first_run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f294c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir = output_dir, \n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        \n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-5,\n",
    "    \n",
    "        #Keep precision at 32bit for training\n",
    "        bf16 = True,\n",
    "        fp16 = False, \n",
    "        #Perform eval every save_steps\n",
    "        do_eval = True,\n",
    "        #Saving based on number of steps\n",
    "        save_strategy = 'steps',\n",
    "        evaluation_strategy = 'steps',\n",
    "        #Number of steps model checkpoints will be saved and evaluated\n",
    "        save_steps = 1000,\n",
    "        eval_steps = 1000,\n",
    "        #Start reporting loss\n",
    "        logging_steps = 1,\n",
    "        #Use paged optimizer for memory efficiency\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "        #2 epochs\n",
    "        num_train_epochs = 2, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72b4face",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments_test = TrainingArguments(\n",
    "        output_dir = output_dir, \n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        \n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-5,\n",
    "    \n",
    "        #Keep precision at 32bit for training\n",
    "        bf16 = True,\n",
    "        fp16 = False, \n",
    "        #Perform eval every save_steps\n",
    "        do_eval = True,\n",
    "        #Saving based on number of steps\n",
    "        save_strategy = 'steps',\n",
    "        evaluation_strategy = 'steps',\n",
    "        #Number of steps model checkpoints will be saved and evaluated\n",
    "        save_steps = 10,\n",
    "        eval_steps = 5,\n",
    "        max_steps = 30,\n",
    "        #Start reporting loss\n",
    "        logging_steps = 1,\n",
    "        #Use paged optimizer for memory efficiency\n",
    "        optim = \"paged_adamw_8bit\"\n",
    "        #2 epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94978194",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = validation_dataset.shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "556e9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:34<00:00, 286.05 examples/s]\n",
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset= val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    args = training_arguments,\n",
    "    peft_config = config,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2001' max='9762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2001/9762 1:24:10 < 5:26:49, 0.40 it/s, Epoch 0.41/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.528900</td>\n",
       "      <td>0.646747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='821' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 821/1250 07:15 < 03:47, 1.88 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84cb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env2)",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
