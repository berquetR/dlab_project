{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651adc3c",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294bdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/berquet/conda/envs/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import transformers\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca469aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a5f5",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3710484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"berquetR/dlab_project_optimal_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f588910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "264f4cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'],\n",
       "    num_rows: 26193\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1250c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_prompt = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88a8af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af89eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_prompt = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be21b8",
   "metadata": {},
   "source": [
    "### Build Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "463584c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format a row according to your fine-tuning requirements\n",
    "def format_row(row):\n",
    "    input_data = {\n",
    "        \"Source\": row['current_page'], \n",
    "        \"Candidates\": row['current_page_links'], \n",
    "        \"Target\": row['target']\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"You are a knowledge discovery expert familiar with the Wikipedia link structure and your objective is to play the game of Wikispeedia: https://dlab.epfl.ch/wikispeedia/play/.\n",
    "##Goal \n",
    "Given two Wikipedia articles, a source and a target, your goal is to reach the target article starting from the source article in as few clicks as possible. For the articles you are given this is always possible.\n",
    "\n",
    "##Constraint \n",
    "You should exclusively follow the links present in the articles that you encounter along the way.\n",
    "\n",
    "##Fine-grained instructions \n",
    "1. While the overall goal is to find a path from a source to a target article, you will proceed step by step.\n",
    "2. Given outgoing links from the source article as candidates, you should select the candidate that takes you closer to the target article. Use your knowledge of the \"expected\" Wikipedia link structure and relatedness between articles to identify the candidate that takes you closer to the target.\n",
    "3. Choose **only** from the provided candidates.\n",
    "4. Do not provide an algorithm, code to solve the task, or explanation just provide the link the choose among candidates.\n",
    "6. Even though the proposed links are not related to the target you should **always** choose a link.\n",
    "\n",
    "##Input \n",
    "{json.dumps(input_data, indent=4)}\n",
    "\n",
    "##Output\n",
    "You should only respond in the JSON format as described below\n",
    "\n",
    "\n",
    "Output format:\n",
    "\n",
    "\"Output\" : \n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dfe5cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26193/26193 [00:04<00:00, 5424.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the formatting function to each row\n",
    "test_dataset = test_dataset.map(lambda x: {\"text\": format_row(x)})\n",
    "\n",
    "# You might want to remove the old columns and keep only 'text'\n",
    "test_dataset = test_dataset.remove_columns(['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c532de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78088/78088 [00:21<00:00, 3575.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda x: {\"text\": format_row(x)})\n",
    "\n",
    "# You might want to remove the old columns and keep only 'text'\n",
    "train_dataset = train_dataset.remove_columns(['source', 'target', 'current_page', 'current_page_links', 'next_page', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16572fcc",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde6135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c50e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:34<00:00, 31.63s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a3c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559b20f",
   "metadata": {},
   "source": [
    "### Test finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51f4904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23d9e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = test_dataset[index]\n",
    "input_tok = tokenizer([input['text']],return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6b8998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a knowledge discovery expert familiar with the Wikipedia link structure and your objective is to play the game of Wikispeedia: https://dlab.epfl.ch/wikispeedia/play/.\n",
      "##Goal \n",
      "Given two Wikipedia articles, a source and a target, your goal is to reach the target article starting from the source article in as few clicks as possible. For the articles you are given this is always possible.\n",
      "\n",
      "##Constraint \n",
      "You should exclusively follow the links present in the articles that you encounter along the way.\n",
      "\n",
      "##Fine-grained instructions \n",
      "1. While the overall goal is to find a path from a source to a target article, you will proceed step by step.\n",
      "2. Given outgoing links from the source article as candidates, you should select the candidate that takes you closer to the target article. Use your knowledge of the \"expected\" Wikipedia link structure and relatedness between articles to identify the candidate that takes you closer to the target.\n",
      "3. Choose **only** from the provided candidates.\n",
      "4. Do not provide an algorithm, code to solve the task, or explanation just provide the link the choose among candidates.\n",
      "6. Even though the proposed links are not related to the target you should **always** choose a link.\n",
      "\n",
      "##Input \n",
      "{\n",
      "    \"Source\": \"Agriculture\",\n",
      "    \"Candidates\": \"['Food', 'History of the world', 'Germany', 'Horse', 'Tea', 'Indonesia', 'Sweden', 'Virgil', 'Food', 'Flower', 'Fertilizer', 'Sugar', 'Ethanol', 'Alcohol', 'Cotton', 'Biodiesel', 'Tobacco', 'Irrigation', 'Indonesia', 'Fossil fuel', 'Soil', 'Nitrogen', 'Phosphorus', 'Fertile Crescent', 'Mesopotamia', 'Barley', 'Pea', 'Egypt', 'Rice', 'Sumer', 'Irrigation', 'Maize', 'Potato', 'Andes', 'South America', 'Sumer', 'Middle Ages', 'Maize', 'Potato', 'Cocoa', 'Tobacco', 'Coffee', 'Cereal', 'Vegetable', 'Milk', 'Fruit', 'Meat', 'Fish', 'Egg (food)', 'Maize', 'Wheat', 'Rice', 'Potato', 'Soybean', 'Barley', 'Nutrition', 'Maize', 'South America', 'Africa', 'Egypt', 'France', 'Fish', 'Beekeeping', 'Honey', 'Botany', 'Nitrogen', 'Phosphorus', 'River', 'Arable land', 'Soil', 'Ammonia', 'Weed', 'Soil salination', 'Global warming', 'Maize', 'Sorghum', 'Wheat']\",\n",
      "    \"Target\": \"Oat\"\n",
      "}\n",
      "\n",
      "##Output\n",
      "You should only respond in the JSON format as described below\n",
      "\n",
      "\n",
      "Output format:\n",
      "\n",
      "\"Output\" : \n",
      "{\n",
      "    \"Step\" : 1,\n",
      "    \"Link\" : \"Barley\",\n",
      "    \"Reason\" : \"Barley is\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = (tokenizer.decode(model.generate(**input_tok, max_new_tokens=30)[0], skip_special_tokens=True))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d54ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_904/673753799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_no_prompt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_page_links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "test_no_prompt[index]['current_page_links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a684e8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_word_in_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mis_word_in_links\u001b[49m(out_extracted , test_no_prompt[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_page_links\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_word_in_links' is not defined"
     ]
    }
   ],
   "source": [
    "is_word_in_links(out_extracted , test_no_prompt[index]['current_page_links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2520b5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'China'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_no_prompt[index]['next_page']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env2)",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
